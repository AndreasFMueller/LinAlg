%
% orthonrmalbasis.tex -- Orthonormierte Basis und Gram Schmidt
%
% (c) 2018 Prof Dr Andreas Müller, Hochschule Rapperswil
%
\section{Skalarprodukt und Basis\label{section:orthonormalbasis}}
\rhead{Skalarprodukt und Basis}
Bei der Berechnung des Skalarproduktes in Komponenten in der Standardbasis
hat sich gezeigt, dass eine Basis aus Einheitsvektoren, die zusätzlich
aufeinander senkrecht stehen, besonders gut für die Arbeit mit dem
Skalarprodukt geeignet ist.
Nicht immer hat man allerdings eine solch bequeme Basis.
In diesemAbschnitt sollen die Vorzüge einer solchen Basis nochmals
herausgearbeitet werden und es soll gezeigt werden, wie man aus
einer beliebigen Basis immer eine passende Basis aus orthogonalen
Einheitsvektoren machen kann.
Schliesslich wird gezeigt, wie sich das Skalarprodukt in einer beliebigen
Basis schreiben lässt.

%
% Orthonormalbasis
%
\subsection{Orthonormalbasis}
Die Vektoren $\vec e_i$ stehen senkrecht aufeinander und haben
Länge $1$.
Der Vektor
\[
\vec v
=
\begin{pmatrix}v_1\\v_2\\v_3\end{pmatrix}
\]
lässt sich mit Hilfe des Skalarproduktes als Summe von Vielfachen
der Vektoren $\vec e_i$ schreiben.
Es ist nämlich $v_i=\vec v\cdot\vec e_i$, also
\[
\vec v
=
\begin{pmatrix}v_1\\v_2\\v_3\end{pmatrix}
=
v_1\vec e_1
+
v_2\vec e_2
+
v_3\vec e_3
=
(\vec v\cdot \vec e_1)\vec e_1
+
(\vec v\cdot \vec e_2)\vec e_2
+
(\vec v\cdot \vec e_3)\vec e_3
\]
Dies funktioniert aber nicht nur für die Vektoren $\vec e_i$.
Seien $\vec b_1$, $\vec b_2$ und $\vec b_3$ drei aufeinander senkrecht
stehende Vektoren der Länge $1$.
Mit dem Skalarprodukt kann man dies durch
\[
\vec b_i\cdot\vec b_j=\begin{cases}
0&\qquad i\ne j\\
1&\qquad i=j
\end{cases}
\]
ausdrücken.
Versucht man den den Vektor $\vec v$ als Linearkombination
der Vektoren $\vec b_i$ zu schreiben, also
\[
\vec v
=
v_1'\vec b_1
+
v_2'\vec b_2
+
v_3'\vec b_3
\]
Berechnet man jetzt das Skalarprodukt von $\vec v$ mit $\vec b_i$,
findet man
\begin{align*}
\vec v\cdot \vec b_i
&=
(
v_1'\vec b_1
+
v_2'\vec b_2
+
v_3'\vec b_3
)\cdot
\vec b_i
\\
&=
v_1'\vec b_1\cdot\vec b_i
+
v_2'\vec b_2\cdot\vec b_i
+
v_3'\vec b_3\cdot\vec b_i
\\
&=v_i'
\end{align*}
weil alle Skalarprodukte verschwinden ausser zwischen
zwei gleichen Vektoren.

\begin{definition} Die Koeffizienten der Einheitsmatrix
\[
\delta_{ij}=
\begin{cases}
0&\qquad i\ne j\\
1&\qquad i=j
\end{cases}
\]
heisst {\em Kronecker-Delta}.
\end{definition}

\begin{definition}
$n$ Vektoren $\vec b_i$ heissen orthonormiert, wenn gilt
\[
\vec b_i\cdot\vec b_j=\delta_{ij}.
\]
\end{definition}

\begin{satz}
Sind die Vektoren $\vec b_i$ orthonormiert, dann kann man jeden
Vektor $\vec v$ als Linearkombination der Vektoren $\vec b_i$
\[
\vec v=
(\vec v\cdot\vec b_1)\vec b_1
+
(\vec v\cdot\vec b_2)\vec b_2
+
(\vec v\cdot\vec b_3)\vec b_3
\]
schreiben.
Diese Darstellung ist eindeutig.
\end{satz}

\begin{proof}[Beweis]
Es ist nur noch zu beweisen, dass es nur eine solche Darstellung als
Linearkombination gibt.
Gäbe es zwei Darstellungen, also
\begin{align*}
\vec v
&=
v_1'\vec b_1+
v_2'\vec b_2+
v_3'\vec b_3\\
&=
v_1''\vec b_1+
v_2''\vec b_2+
v_3''\vec b_3,
\end{align*}
können wir die Differenz bilden:
\[
0=
(v_1'-v_1'')\vec b_1
+
(v_2'-v_2'')\vec b_2
+
(v_3'-v_3'')\vec b_3
\]
Das Skalarprodukt mit $b_i$ ergibt dann
\[
0=(v_i'-v_i'')\quad\Rightarrow\quad v_i'=v_i''.
\]
Da wir $i$ beliebig wählen können folgt, dass die
Koeffizienten $v_i'$ und $v_i''$ übereinstimmen.
\end{proof}

%
% Orthonormalisierung 
%
\subsection{Orthonormalisierung}
Für orthonormierte Vektoren ist es besonders einfach, eine Darstellung
eines beliebigen Vektors als Linearkombination zu finden.
Es ist daher sicher nützlich, aus einer Menge von Vektoren
$\{\vec a_1,\vec a_2,\vec a_3\}$
eine neue Menge von Vektoren zu konstruieren, die sich von der gegeben
möglichst wenig
unterscheidet, aber dennoch aus orthonormierten Vektoren besteht.

\begin{satz}[Gram-Schmidt]
\label{satz-gram-schmidt}
Seien $\{\vec a_1,\vec a_2,\vec a_3\}$ linear unabhängige Vektoren.
Dann gibt es orthonormierte Vektoren $\{\vec b_1,\vec b_2,\vec b_3\}$ so,
dass $b_k$ aus $a_1,\dots,a_k$ linear kombiniert werden kann, für jedes $k$.
Die $b_i$ lassen sich wie folgt berechnen
\begin{align*}
\vec b_1&=\frac1{|\vec a_1|}a_1\\
\vec b_2&=
\frac{
\vec a_2-(\vec a_2\cdot \vec b_1)\vec b_1
}{
|\vec a_2-(\vec a_2\cdot \vec b_1)\vec b_1|
}
\\
\vec b_3
&=
\frac{
\vec a_3-(\vec a_3\cdot \vec b_1)\vec b_1-(\vec a_3\cdot\vec b_2)\vec b_2
}{
|
\vec a_3-(\vec a_3\cdot \vec b_1)\vec b_1-(\vec a_3\cdot\vec b_2)\vec b_2
|
}
\\
&\phantom{=}\vdots\\
\vec b_k&=\frac{\vec a_k-(\vec a_k\cdot \vec b_1)\vec b_1-(\vec a_k\cdot \vec b_2)\vec b_2-\dots-(\vec a_k\cdot \vec b_{k-1})\vec b_{k-1}}{|\vec a_k-(\vec a_k\cdot \vec b_1)\vec b_1-(\vec a_k\cdot \vec b_2)\vec b_2-\dots-(\vec a_k\cdot \vec b_{k-1})\vec b_{k-1}|}
\end{align*}
Das Verfahren lässt sich offenbar auf eine beliebige Zahl linear
unabhängiger $n$-dimensionaler Vektoren verallgemeinern.
\end{satz}

Auf die Reihenfolge der Vektoren kommt es entscheidend an, wie die
folgenden zwei Beispiele zeigen
\begin{beispiel}
Die Vektoren
\[
\vec a_1=\begin{pmatrix}1\\0\\0\end{pmatrix},\qquad
\vec a_2=\begin{pmatrix}1\\1\\0\end{pmatrix},\qquad
\vec a_3=\begin{pmatrix}1\\1\\1\end{pmatrix}
\]
sind zu orthonormieren.

Die Formeln aus Satz~\ref{satz-gram-schmidt} liefern folgende Vektoren:
\begin{align*}
\vec b_1&=\frac{\vec a_1}{|\vec a_1|}=\begin{pmatrix}1\\0\\0\end{pmatrix}\\
\vec b_2&=
\frac{
\vec a_2-(\vec a_2\cdot \vec b_1)\vec b_1
}{
|\vec a_2-(\vec a_2\cdot \vec b_1)\vec b_1|
}
=
\frac{
\begin{pmatrix}1\\1\\0\end{pmatrix}-1\cdot\begin{pmatrix}1\\0\\0\end{pmatrix}
}{\dots}=\begin{pmatrix}0\\1\\0\end{pmatrix}\\
\\
\vec b_3&=
\frac{\vec a_3 -(\vec a_3\cdot \vec b_1)\vec b_1-(\vec a_3\cdot\vec b_2)\vec b_2}{\dots}
=\frac{\begin{pmatrix}1\\1\\1\end{pmatrix}-1\cdot \begin{pmatrix}1\\0\\0\end{pmatrix}-1\cdot\begin{pmatrix}0\\1\\0\end{pmatrix}
}{\dots}=\begin{pmatrix}0\\0\\1\end{pmatrix}
\end{align*}
Man findet also genau die Vektoren der Standardbasis.
\end{beispiel}

\begin{beispiel}
Die Vektoren
\[
\vec a_1=\begin{pmatrix}1\\0\\0\end{pmatrix},\qquad
\vec a_2=\begin{pmatrix}1\\1\\1\end{pmatrix},\qquad
\vec a_3=\begin{pmatrix}1\\1\\0\end{pmatrix}
\]
sind zu orthonormieren.

Dieses Beispiel unterscheidet sich vom vorangegangenen nur
durch die Reihenfolge der Vektoren.
Wieder können die Formeln von Satz~\ref{satz-gram-schmidt} angewandt werden:
\begin{align*}
\vec b_1&=\frac{\vec a_1}{|\vec a_1|}=\begin{pmatrix}1\\0\\0\end{pmatrix}
\\
\vec b_2
&=
\frac{\vec a_2-(\vec a_2\cdot \vec b_1)\vec b_1}{\dots}
=
\frac{\begin{pmatrix}1\\1\\1\end{pmatrix}-1\cdot\begin{pmatrix}1\\0\\0\end{pmatrix}}{\dots}=\frac1{\sqrt{2}}\begin{pmatrix}0\\1\\1\end{pmatrix}
\\
\vec b_3
&=
\frac{\vec a_3-(\vec a_3\cdot\vec b_1)\vec b_1-(\vec a_3\cdot\vec b_2)\vec b_2}{\dots}
=\frac{\displaystyle\begin{pmatrix}1\\1\\0\end{pmatrix}-1\cdot\begin{pmatrix}1\\0\\0\end{pmatrix}-\frac1{\sqrt{2}}\cdot\frac1{\sqrt{2}}\begin{pmatrix}0\\1\\1\end{pmatrix} }{\cdots}
=\frac{1}{\sqrt{2}}\begin{pmatrix}0\\1\\-1\end{pmatrix}.
\end{align*}
Die gefundenen Vektoren sind völlig verschiedenen von den Vektoren
im vorangegangenen Beispiel.
\end{beispiel}

%
% Skalarprodukt und Matrixprodukt
%
\subsection{Skalarprodukte und Matrixprodukt}
Das in der Vektorgeometrie definiert Skalarprodukt kann in beliebig
vielen Dimensionen definiert werden.
Zwei Vektoren $v$ und $u$ in $\mathbb R^n$ haben
das Skalarprodukt
\[
u\cdot v=
\begin{pmatrix}u_1\\\vdots\\u_n\end{pmatrix}
\cdot
\begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}
=u_1v_1+\dots+u_nv_n=u^tv.
\]
Alle bisher entwickelten Anwendungen des Skalarproduktes lassen sich sofort
auf den $n$-di\-men\-sio\-nalen Raum übertragen.
Zum Beispiel ist die Länge eines Vektors definiert als
\[
|v|=\sqrt{v^tv}.
\]
Programme wie Octave oder Matlab brauchen daher keine spezielle Notation
für das Skalarprodukt, da mit Transposition und Matrixprodukt
bereits alles vorhanden ist, um das Skalarprodukt auszurechnen.
In Matlab-Notation ist das Skalarprodukt der Spaltenvektoren
\texttt{u}
und
\texttt{v}
ist 
\texttt{u'*v}.

In den Anwendungen findet man aber weitere Beispiel von
Skalarprodukt-ähnlichen Grössen, zum Beispiel das Trägheitsmoment.
Die Rotationsenergie eines starren Körpers kann aus dem Vektor $\omega$
der Winkelgeschwindigkeit mit der (symmetrischen) Matrix des
Trägheitsmomentes $\Theta$
berechnet werden:
$
E=\frac12 \omega^t\Theta \omega.
$
Die allgemeinste Form eines Skalarproduktes verwendet daher eine
symmetrische Matrix $M$, also eine Matrix mit der Eigenschaft
$M=M^t$, und berechnet das Skalarprodukt zweier Vektoren $u$ und $v$
als $u^tMv$.
\begin{definition}
Ist $M$ eine symmetrische $n\times n$-Matrix, dann ist
\[
u^t Mv
=
\langle u,v\rangle_M
=
\langle u,v\rangle
\]
das zugehörige verallgemeinerte Skalarprodukt.
\end{definition}
Das gewöhnliche Skalarprodukt ist in dieser allgemeinen Definition
als der Fall $M=E$ enthalten.
Die Matrix
\[
M=\begin{pmatrix}
1& 0& 0& 0\\
0&-1& 0& 0\\
0& 0&-1& 0\\
0& 0& 0&-1
\end{pmatrix}
\]
führt auf das sogenannte Minkowski-Skalarprodukt, so benannt nach
Hermann Minkowski, der ab 1896 Professor an der ETH war und bei dem
Einstein studiert hat.
Es erlangt in der speziellen und allgemeinen Relativitätstheorie von
Einstein fundamentale Bedeutung.

%
% Basiswechsel
%
\subsection{Basiswechsel}
Sei wieder $T$ die Matrix, die Koordinaten von der Basis $B'$ in die
Basis $B$ umrechnet.
Um das Skalarprodukt bezüglich der Basis $B'$ zu berechnen,
müssen die Vektoren zuerst mit $T$ in die Basis $B$ umgerechnet werden,
so dass dann die Formel für das Skalarprodukt in der Basis $B$
verwendet werden kann:
\[
(T\xi)^tT\eta=\xi^tT^tT\eta
\]
Die Matrix $T^tT$ ist symmetrisch, der ``Spezialfall'' eines Skalarproduktes,
welches mit Hilfe einer symmetrischen Matrix definiert worden ist,
ist also unvermeidlich, wenn man die Basis wechseln will.

