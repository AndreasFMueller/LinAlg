%
% orthonrmalbasis.tex -- Orthonormierte Basis und Gram Schmidt
%
% (c) 2018 Prof Dr Andreas Müller, Hochschule Rapperswil
%
\section{Skalarprodukt und Basis\label{section:orthonormalbasis}}
Bei der Berechnung des Skalarproduktes in Komponenten in der Standardbasis
hat sich gezeigt, dass eine Basis aus Einheitsvektoren, die zusätzlich
aufeinander senkrecht stehen, besonders gut für die Arbeit mit dem
Skalarprodukt geeignet ist.
Nicht immer hat man allerdings eine solch bequeme Basis.
In diesemAbschnitt sollen die Vorzüge einer solchen Basis nochmals
herausgearbeitet werden und es soll gezeigt werden, wie man aus
einer beliebigen Basis immer eine passende Basis aus orthogonalen
Einheitsvektoren machen kann.
Schliesslich wird gezeigt, wie sich das Skalarprodukt in einer beliebigen
Basis schreiben lässt.

\subsection{Orthonormalbasis}
Die Vektoren $\vec e_i$ stehen senkrecht aufeinander und haben
Länge $1$.
Der Vektor
\[
\vec v
=
\begin{pmatrix}v_1\\v_2\\v_3\end{pmatrix}
\]
lässt sich mit Hilfe des Skalarproduktes als Summe von Vielfachen
der Vektoren $\vec e_i$ schreiben.
Es ist nämlich $v_i=\vec v\cdot\vec e_i$, also
\[
\vec v
=
\begin{pmatrix}v_1\\v_2\\v_3\end{pmatrix}
=
v_1\vec e_1
+
v_2\vec e_2
+
v_3\vec e_3
=
(\vec v\cdot \vec e_1)\vec e_1
+
(\vec v\cdot \vec e_2)\vec e_2
+
(\vec v\cdot \vec e_3)\vec e_3
\]
Dies funktioniert aber nicht nur für die Vektoren $\vec e_i$.
Seien $\vec b_1$, $\vec b_2$ und $\vec b_3$ drei aufeinander senkrecht
stehende Vektoren der Länge $1$.
Mit dem Skalarprodukt kann man dies durch
\[
\vec b_i\cdot\vec b_j=\begin{cases}
0&\qquad i\ne j\\
1&\qquad i=j
\end{cases}
\]
ausdrücken.
Versucht man den den Vektor $\vec v$ als Linearkombination
der Vektoren $\vec b_i$ zu schreiben, also
\[
\vec v
=
v_1'\vec b_1
+
v_2'\vec b_2
+
v_3'\vec b_3
\]
Berechnet man jetzt das Skalarprodukt von $\vec v$ mit $\vec b_i$,
findet man
\begin{align*}
\vec v\cdot \vec b_i
&=
(
v_1'\vec b_1
+
v_2'\vec b_2
+
v_3'\vec b_3
)\cdot
\vec b_i
\\
&=
v_1'\vec b_1\cdot\vec b_i
+
v_2'\vec b_2\cdot\vec b_i
+
v_3'\vec b_3\cdot\vec b_i
\\
&=v_i'
\end{align*}
weil alle Skalarprodukte verschwinden ausser zwischen
zwei gleichen Vektoren.

\begin{definition} Die Koeffizienten der Einheitsmatrix
\[
\delta_{ij}=
\begin{cases}
0&\qquad i\ne j\\
1&\qquad i=j
\end{cases}
\]
heisst {\em Kronecker-Delta}.
\end{definition}

\begin{definition}
$n$ Vektoren $\vec b_i$ heissen orthonormiert, wenn gilt
\[
\vec b_i\cdot\vec b_j=\delta_{ij}.
\]
\end{definition}

\begin{satz}
Sind die Vektoren $\vec b_i$ orthonormiert, dann kann man jeden
Vektor $\vec v$ als Linearkombination der Vektoren $\vec b_i$
\[
\vec v=
(\vec v\cdot\vec b_1)\vec b_1
+
(\vec v\cdot\vec b_2)\vec b_2
+
(\vec v\cdot\vec b_3)\vec b_3
\]
schreiben.
Diese Darstellung ist eindeutig.
\end{satz}

\begin{proof}[Beweis]
Es ist nur noch zu beweisen, dass es nur eine solche Darstellung als
Linearkombination gibt.
Gäbe es zwei Darstellungen, also
\begin{align*}
\vec v
&=
v_1'\vec b_1+
v_2'\vec b_2+
v_3'\vec b_3\\
&=
v_1''\vec b_1+
v_2''\vec b_2+
v_3''\vec b_3,
\end{align*}
können wir die Differenz bilden:
\[
0=
(v_1'-v_1'')\vec b_1
+
(v_2'-v_2'')\vec b_2
+
(v_3'-v_3'')\vec b_3
\]
Das Skalarprodukt mit $b_i$ ergibt dann
\[
0=(v_i'-v_i'')\quad\Rightarrow\quad v_i'=v_i''.
\]
Da wir $i$ beliebig wählen können folgt, dass die
Koeffizienten $v_i'$ und $v_i''$ übereinstimmen.
\end{proof}

%\subsection{Verallgemeinertes Skalarprodukt}

%\begin{definition}
%Ein symmetrische Matrix $g_{ij}$ definiert ein allgemeines Skalarprodukt
%$$g(x,y)=\sum_{i,j=1}^ng_{ij}x_iy_i$$
%der Vektoren
%$$x=\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix}\qquad\text{und}
%\qquad y=\begin{pmatrix}y_1\\\vdots\\y_n\end{pmatrix}.$$
%\end{definition}
%Das zu Beginn dieses Abschnitts definiert Skalarprodukt ist ein
%verallgemeinertes Skalarprodukt mit der Matrix $I$.

\subsection{Orthonormalisierung}
Für orthonormierte Vektoren ist es besonders einfach, eine Darstellung
eines beliebigen Vektors als Linearkombination zu finden.
Es ist daher sicher nützlich, aus einer Menge von Vektoren
$\{\vec a_1,\vec a_2,\vec a_3\}$
eine neue Menge von Vektoren zu konstruieren, die sich von der gegeben
möglichst wenig
unterscheidet, aber dennoch aus orthonormierten Vektoren besteht.

\begin{satz}[Gram-Schmidt]
\label{satz-gram-schmidt}
Seien $\{\vec a_1,\vec a_2,\vec a_3\}$ linear unabhängige Vektoren.
Dann gibt es orthonormierte Vektoren $\{\vec b_1,\vec b_2,\vec b_3\}$ so,
dass $b_k$ aus $a_1,\dots,a_k$ linear kombiniert werden kann, für jedes $k$.
Die $b_i$ lassen sich wie folgt berechnen
\begin{align*}
\vec b_1&=\frac1{|\vec a_1|}a_1\\
\vec b_2&=
\frac{
\vec a_2-(\vec a_2\cdot \vec b_1)\vec b_1
}{
|\vec a_2-(\vec a_2\cdot \vec b_1)\vec b_1|
}
\\
\vec b_3
&=
\frac{
\vec a_3-(\vec a_3\cdot \vec b_1)\vec b_1-(\vec a_3\cdot\vec b_2)\vec b_2
}{
|
\vec a_3-(\vec a_3\cdot \vec b_1)\vec b_1-(\vec a_3\cdot\vec b_2)\vec b_2
|
}
\\
&\phantom{=}\vdots\\
\vec b_k&=\frac{\vec a_k-(\vec a_k\cdot \vec b_1)\vec b_1-(\vec a_k\cdot \vec b_2)\vec b_2-\dots-(\vec a_k\cdot \vec b_{k-1})\vec b_{k-1}}{|\vec a_k-(\vec a_k\cdot \vec b_1)\vec b_1-(\vec a_k\cdot \vec b_2)\vec b_2-\dots-(\vec a_k\cdot \vec b_{k-1})\vec b_{k-1}|}
\end{align*}
Das Verfahren lässt sich offenbar auf eine beliebige Zahl linear
unabhängiger $n$-dimensionaler Vektoren verallgemeinern.
\end{satz}

Auf die Reihenfolge der Vektoren kommt es entscheidend an, wie die
folgenden zwei Beispiele zeigen
\begin{beispiel}
Die Vektoren
\[
\vec a_1=\begin{pmatrix}1\\0\\0\end{pmatrix},\qquad
\vec a_2=\begin{pmatrix}1\\1\\0\end{pmatrix},\qquad
\vec a_3=\begin{pmatrix}1\\1\\1\end{pmatrix}
\]
sind zu orthonormieren.

Die Formeln aus Satz~\ref{satz-gram-schmidt} liefern folgende Vektoren:
\begin{align*}
\vec b_1&=\frac{\vec a_1}{|\vec a_1|}=\begin{pmatrix}1\\0\\0\end{pmatrix}\\
\vec b_2&=
\frac{
\vec a_2-(\vec a_2\cdot \vec b_1)\vec b_1
}{
|\vec a_2-(\vec a_2\cdot \vec b_1)\vec b_1|
}
=
\frac{
\begin{pmatrix}1\\1\\0\end{pmatrix}-1\cdot\begin{pmatrix}1\\0\\0\end{pmatrix}
}{\dots}=\begin{pmatrix}0\\1\\0\end{pmatrix}\\
\\
\vec b_3&=
\frac{\vec a_3 -(\vec a_3\cdot \vec b_1)\vec b_1-(\vec a_3\cdot\vec b_2)\vec b_2}{\dots}
=\frac{\begin{pmatrix}1\\1\\1\end{pmatrix}-1\cdot \begin{pmatrix}1\\0\\0\end{pmatrix}-1\cdot\begin{pmatrix}0\\1\\0\end{pmatrix}
}{\dots}=\begin{pmatrix}0\\0\\1\end{pmatrix}
\end{align*}
Man findet also genau die Vektoren der Standardbasis.
\end{beispiel}

\begin{beispiel}
Die Vektoren
\[
\vec a_1=\begin{pmatrix}1\\0\\0\end{pmatrix},\qquad
\vec a_2=\begin{pmatrix}1\\1\\1\end{pmatrix},\qquad
\vec a_3=\begin{pmatrix}1\\1\\0\end{pmatrix}
\]
sind zu orthonormieren.

Dieses Beispiel unterscheidet sich vom vorangegangenen nur
durch die Reihenfolge der Vektoren.
Wieder können die Formeln von Satz~\ref{satz-gram-schmidt} angewandt werden:
\begin{align*}
\vec b_1&=\frac{\vec a_1}{|\vec a_1|}=\begin{pmatrix}1\\0\\0\end{pmatrix}
\\
\vec b_2
&=
\frac{\vec a_2-(\vec a_2\cdot \vec b_1)\vec b_1}{\dots}
=
\frac{\begin{pmatrix}1\\1\\1\end{pmatrix}-1\cdot\begin{pmatrix}1\\0\\0\end{pmatrix}}{\dots}=\frac1{\sqrt{2}}\begin{pmatrix}0\\1\\1\end{pmatrix}
\\
\vec b_3
&=
\frac{\vec a_3-(\vec a_3\cdot\vec b_1)\vec b_1-(\vec a_3\cdot\vec b_2)\vec b_2}{\dots}
=\frac{\displaystyle\begin{pmatrix}1\\1\\0\end{pmatrix}-1\cdot\begin{pmatrix}1\\0\\0\end{pmatrix}-\frac1{\sqrt{2}}\cdot\frac1{\sqrt{2}}\begin{pmatrix}0\\1\\1\end{pmatrix} }{\cdots}
=\frac{1}{\sqrt{2}}\begin{pmatrix}0\\1\\-1\end{pmatrix}.
\end{align*}
Die gefundenen Vektoren sind völlig verschiedenen von den Vektoren
im vorangegangenen Beispiel.
\end{beispiel}

