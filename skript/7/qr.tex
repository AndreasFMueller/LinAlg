%
% qr.tex -- QR-Zerlegung
%
% (c) 2009 Prof Dr Andreas Mueller, Hochschule Rapperswil
%
\section{QR-Zerlegung\label{section-qr}}
\rhead{QR-Zerlegung}
\index{QR-Zerlegung}
\index{Zerlegung!QR}
Die Zerlegung in zwei Dreiecksmatrizen mag für das Problem, Gleichungen
zu lösen, adäquat sein.
Für viele andere Problem wünscht man
sich von den Faktoren weitere oder andere Eigenschaften, zum Beispiel
Orthogonalität.
Die QR-Zerlegung liefert eine Matrix mit orthonormierten Spalten.

\subsection{QR-Zerlegung und Gram-Schmidt-Orthonormalisierung}
Betrachten wir nochmals den Orthonormalisierungsprozess.
Im $k$-ten Schritt erzeugt er den Vektor $b_k'$ aus $b_1',\dots,b_{k-1}'$
und $b_k$ nach der Formel
\[
b_k'=\frac{
b_k-(b_1'\cdot b_k)b_1'-\dots-(b_{k-1}'\cdot b_k)b_{k-1}'
}{|
b_k-(b_1'\cdot b_k)b_1'-\dots-(b_{k-1}'\cdot b_k)b_{k-1}'
|}
\]
Da sich die Vektoren $b_1',\dots,b_{k-1}'$ aus 
$b_1,\dots,b_{k-1}$ kombinieren lassen, gibt es Koeffizienten $r_{ik}$
so, dass
\[
b_k'=r_{1k}b_1+\dots r_{kk}b_k
\]
Schreiben wir wieder $\tilde B$ für die Matrix, deren Spalten die
$b_i$ sind, ist dies gleichbedeutend mit
\[
b_k'=\tilde B\begin{pmatrix}r_{1k}\\\vdots\\r_{kk}\\\vdots\\0\end{pmatrix}
\]
Man kann also alle Koeffizienten $r_{ki}$ in eine $n\times n$ Matrix $R$
zusammenfassen:
\[
R=\begin{pmatrix}
r_{11}&r_{12}&r_{13}&\dots &r_{1k}\\
0     &r_{22}&r_{23}&\dots &r_{2k}\\
0     &0     &r_{33}&\dots &r_{3k}\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0     &0     &0     &\dots &r_{kk}\\
\end{pmatrix}.
\]
$R$ ist eine invertierbare
obere Dreiecksmatrix, weil keines der
Diagonalelemente
\[
r_{ii}=\frac1{|
b_i-(b_1'\cdot b_i)b_1'-\dots-(b_{i-1}'\cdot b_i)b_{i-1}'
|}
\]
verschwindet.
Die Matrix $\tilde B'$ mit Spalten $b_i'$ entsteht als Produkt aus $\tilde B$
und $R$:
\[
\tilde B'=\tilde BR
\quad\Rightarrow\quad
\tilde B=\tilde B'R^{-1}
\]
Da auch $R^{-1}$ eine Dreiecksmatrix ist, haben wir eine Zerlegung der
Matrix $\tilde B$ in eine Matrix $\tilde B'$ mit orthonormierten
Spalten und eine obere Dreiecksmatrix erreicht.

\begin{satz}[QR-Zerlegung]
\index{QR-Zerlegung}
\index{Zerlegung!QR}
Ist $A$ eine $m\times n$-Matrix mit linear unabhängigen Spalten,
dann gibt es eine $m\times n$-Matrix $Q$ mit orthonormalen Spalten
und eine $n\times n$-Dreiecksmatrix $R$ so, dass $A=QR$.
\end{satz}

Im Spezialfall $n=m$, also für quadratische Matrizen, liefert der
Algorithmus eine orthogonale Matrix $Q$ und obere Dreiecksmatrix eine $R$
so, dass $A=QR$.
Da $Q^{-1}=Q^t$ ist, kann man $R=AQ^t$ bestimmen,
was die Berechnung von $R$ vereinfacht.
Wieder ist $\det(A)=\det(Q)\det(R)=\pm\det(R)$.

\begin{beispiel}
Man finde die QR-Zerlegung von 
\[
A=\begin{pmatrix}1&0&0\\1&1&0\\1&1&1\end{pmatrix}.
\]
Für die Matrix $Q$ kann man das Resultat der Orthonormalisierung
verwenden, welches wir bereits in einem früheren Beispiel gefunden
haben:
\[
Q=
\begin{pmatrix}
\frac1{\sqrt{3}}&\frac{-2}{\sqrt{6}}&0\\
\frac1{\sqrt{3}}&\frac{1}{\sqrt{6}}&\frac{-1}{\sqrt{2}}\\
\frac1{\sqrt{3}}&\frac{1}{\sqrt{6}}&\frac{1}{\sqrt{2}}
\end{pmatrix}
\]
Dann muss $R=Q^{-1}A=Q^tA$ sein:
\[
R=
\begin{pmatrix}
\frac1{\sqrt{3}}&\frac1{\sqrt{3}}&\frac1{\sqrt{3}}\\
\frac{-2}{\sqrt{6}}&\frac{1}{\sqrt{6}}&\frac{1}{\sqrt{6}}\\
0&\frac{-1}{\sqrt{2}}&\frac{1}{\sqrt{2}}
\end{pmatrix}
\begin{pmatrix}1&0&0\\1&1&0\\1&1&1\end{pmatrix}
=
\begin{pmatrix}
\sqrt{3}&\frac{2}{\sqrt{3}}&\frac{1}{\sqrt{3}}\\
0&\frac2{\sqrt{6}}&\frac1{\sqrt{6}}\\
0&0&\frac1{\sqrt{2}}
\end{pmatrix}
\]
\end{beispiel}

%
% Geometrische Interpretation der QR-Zerlegung
%
\subsection{QR-Zerlegung zur Analyse einer linearen Abbildung}
Eine $3\times 3$-Matrix beschreibt eine linear Abbildung
$\mathbb{R}^3\to\mathbb{R}^3$.
In diesem Abschnitt soll gezeigt werden, wie die QR-Zerlegung von $A$
dazu verwendet werden kann zu verstehen, aus welchen einfacheren
Abbildungen sich die durch $A$ beschriebene Abbildung zusammensetzt.

Sei also im folgenden $A=QR$ die QR-Zerlegung der Matrix $A$.
Die Definition der QR-Zerlegung macht keine Voraussetzungen über das
Vorzeichen der Determinanten von $Q$ oder $R$.
Sollte $\det(Q)<0$ sein, kann man die erste Spalte von $Q$ mit einem
negativen Vorzeichen versehen wie auch das Element $r_{11}$ in der
linken oberen Ecke von $R$.
Diese neuen Matrizen $Q$ und $R$ stellen immer noch eine QR-Zerlegung
der Matrix $A$ dar, aber die Determinanten von $Q$ und $R$ haben jetzt
entgegengesetztes Vorzeichen.
Insbesondere kann man also immer annehmen, dass eine QR-Zerlegung
mit $\det(Q)=1$ vorliegt.

Die Matrix $R$ ist eine obere Dreiecksmatrix.
Wir überlegen uns zunächst, dass man es immer so einrichten kann,
dass höchstens eines der Diagonalelemente von $R$ negativ ist.
Wenn nämlich mehr als zwei Diagonalelemente negativ sind, kann man
$R$ von links mit einer Drehmatrix multiplizieren, die das Vorzeichen
der beiden Diagonalelemente kehrt.
Sind zum Beispiel die ersten zwei Diagonalelemente von $R$ negativ,
dann kann man $R$ von links mit der Drehmatrix
\[
D_{z,180^\circ}
=
\begin{pmatrix}
-1& 0& 0\\
 0&-1& 0\\
 0& 0& 1
\end{pmatrix}
,\qquad
D_{z,180^circ}R
=
\begin{pmatrix}
-r_{11}&-r_{12}&-r_{13}\\
   0   &-r_{12}&-r_{13}\\
   0   &   0   &\phantom{-}r_{13}
\end{pmatrix}
\]
multiplizieren.
Die Vorzeichen von $r_{11}$ und $r_{22}$ ändern.
Die Matrix $Q$ muss man natürlich ebenfalls durch $QD_{z,180^\circ}$
ersetzen.
Da $D_{7,180^\circ}$ eine Drehmatrix ist, ist das neue $Q$ immer noch
eine Drehmatrix, $Q\in\operatorname{SO}(3)$.
So erhält man wieder eine QR-Zerlegung von $A$, man darf jetzt aber
zusätzlich annehmen, dass nur eines der Diagonalelemente von $R$ negativ
ist.

Die Diagonalelemente von $R$ können in separate Matrizen verschoben werden.
Wir schreiben 
\begin{align*}
R
&=
R
\begin{pmatrix}
r_{11}^{-1}&     0     &     0     \\
     0     &r_{22}^{-1}&     0     \\
     0     &     0     &r_{33}^{-1}
\end{pmatrix}
\begin{pmatrix}
r_{11}&   0  &  0   \\
   0  &r_{22}&  0   \\
   0  &   0  &r_{33}
\end{pmatrix}
\\
&=
\underbrace{
\begin{pmatrix}
1&\frac{r_{12}}{r_{22}}&\frac{r_{13}}{r_{33}}\\
0&            1        &\frac{r_{23}}{r_{33}}\\
0&            0        &            1
\end{pmatrix}
}_{\displaystyle=R_0}
\underbrace{
\begin{pmatrix}
|r_{11}|&0&0\\
  0   &1&0\\
  0   &0&1
\end{pmatrix}
}_{\displaystyle = S_x}
\underbrace{
\begin{pmatrix}
1&0&0\\
0&|r_{22}|&0\\
0&0&1
\end{pmatrix}
}_{\displaystyle = S_y}
\underbrace{
\begin{pmatrix}
1&0&0\\
0&1&0\\
0&0&|r_{33}|
\end{pmatrix}
}_{\displaystyle = S_z}
\underbrace{
\begin{pmatrix}
\operatorname{sign}(r_{11})&0&0\\
0&\operatorname{sign}(r_{22})&0\\
0&0&\operatorname{sign}(r_{33})
\end{pmatrix}
}_{\displaystyle = S}
\end{align*}
Die Vorzeichenfunktion $\operatorname{sign}(x)$ ist definiert durch
\[
\operatorname{sign}(x)
=
\begin{cases}
-1&\quad x<0\\
0&\quad x=0\\
1&\quad x>0.
\end{cases}
\]
In der Matrix $S$ stehen also die Vorzeichen der Diagonalelemente
von $R$.
Da wir weiter oben sichergestellt haben, dass höchstens eines der
Diaogonalelemente von $R$ negativ ist, enthält $S$ höchstens
einen Eintrag $-1$.
Die Matrix $S$ ist also entweder eine Spiegelung an einer der Koordinatenebenen
oder die Einheitsmatrix.

Die Matrizen $S_x$, $S_y$ und $S_z$ sind Streckungen der Koordinatenachsen.
$S_x$ streckt die $x$-Achse um den Faktor $|r_{11}|$, und entsprechend 
streckt $S_y$ die $y$-Achse um den Faktor $|r_{22}|$ und analog für $S_z$.

Die Matrix $R_0$ kann geometrisch ebenfalls gut verstanden werden.
Die erste Spalte von $R_0$ ist der erste Standardbasisvektor, dieser
bleibt also erhalten.
Der zweite Standardbasisvektor wird auf die zweite Spalte abgebildet,
dies verschiebt die Spitze des zweiten Standardbasisvektors um $r_{12}$ in
$x$-Richtung.
Die dritte Spalte schliesslich bedeutet, dass die Spitze des dritten
Standardbasisvektors parallel zur $x$-$y$-Ebenen um $r_{13}$ in $x$-Richtung
und um $r_{23}$ in $y$-Richtung verschoben wird.
Die Matrix $R_0$ beschreibt also eine Zusammensetzung von Scherungen 
in drei Dimensionen.

Damit hat die QR-Zerlegung ermöglicht, die Abbildung $A$ in die Faktoren
\[
A=QR_0S_xS_yS_zS
\]
zu zerlegen.
Darin ist $S$ eine Spiegelung an einer Koordinatenebene, die nur nötig
ist, wenn $\det(A) < 0$ ist.
Dann folgen drei Skalierungen der Koordinatenachsen mit den Matrizen
$S_x$, $S_y$ und $S_z$, diese drei Matrizen vertauschen, es kommt also
nicht auf deren Reihenfolge an.
Anschliessend werden die Scherungen der Matrix $R_0$ angewendet.
Schliesslich wird alles mit der Drehmatrix gedreht\footnote{
Eine animierte Darstellung dieses Prozesses ist im Video
\url{https://youtu.be/8LLx5UydD_U} auf dem MathMan-Youtube-Channel
zu sehen.}.

%
% QR-Zerlegung und Least-Squares
%
\subsection{QR-Zerlegung und Least-Squares}
Die QR-Zerlegung kann dazu verwendet werden, das Least-Squares-Problem
zu lösen.
Dabei wird für ein überbestimmtes Gleichungssystem mit
$m\times n$-Koeffizienten\-matrix $A$ und rechter Seite $b$ ein Vektor $v$
so gesucht,
dass $Ax-b$ minimal wird.
Die in Abschnitt~\ref{section:ueberbestimmt}
gefundene Lösung benötigt die Matrizenprodukte $A^tA$ und $A^tb$, welche
für grosses $m$ aufwendig zu berechnen sind.
Es besteht also die Hoffnung,
dass eine alternative Methode diesen Aufwand reduzieren könnte.

Die geometrische Lösung des Problems aus Abschnitt~\ref{section:ueberbestimmt}
verwendet, dass $Av-b$ senkrecht steht auf $Av$.
Die QR-Zerlegung ist
im Wesentlichen eine Umformulierung des Orthogonalisierungsverfahrens, in den
Matrizen $Q$ und $R$ ist also die Information über Orthogonalität ebenfalls
enthalten.

Genauer: wenn $A=QR$ ist, wobei $Q$ eine orthogonale $m\times m$-Matrix
ist und $R$ eine obere $m\times n$-Dreiecksmatrix, dann bilden die
ersten $n$ Spaltenvektoren von $Q$ eine orthonormierte Basis des
von den Spalten von $A$ aufgespannten Raumes.
Wir nennen diese Vektoren
$Q_{\|}$.
Die nachfolgenden Spaltenvektoren
in $Q$ stehen alle senkrecht auf $Av$, wir nennen sie $Q_{\perp}$.
Mit diesen Bezeichnungen können wir die Matrix $Q$ schreiben als
\[
Q=\begin{pmatrix}
*     &\dots &     *&      *&\dots   &*     \\
\vdots&      &\vdots&\vdots &        &\vdots\\
\vdots&Q_{\|}&\vdots&\vdots&Q_{\perp}&\vdots\\
\vdots&      &\vdots&\vdots &        &\vdots\\
*     &\dots &     *&      *&\dots   &*     
\end{pmatrix}.
\]
Die Matrix $R$ liefert die Umrechnung von $v$ in die Basis $Q$.

Gesucht ist ein jetzt Vektor $v$ so, dass
$Av-b$ senkrecht steht auf dem von $A$ aufgespannten Raum.
Offenbar ist dies in der Basis $Q$ viel einfacher auszudrücken:
gesucht ist ein Vektor $v$ so, dass $Av-b$ im von $Q_{\perp}$
aufgespannten Raum liegt, oder so, dass $Av-b$ verschwindende Koordinaten
für die Basisvektoren $Q_{\|}$ hat.

Dafür müssen wir die Koordinaten eines Vektors in der Basis $Q$
berechnen können.
Sind $\xi$ diese Koordinaten des Vektors $v$,
dann gilt offenbar $Q\xi = v$.
Da $Q$ orthogonal ist, ist $Q^{-1}=Q^t$, es ist also $\xi=Q^tv$.
Insbesondere sind die Koordinaten von $b$ in der
Matrix $Q$ gegeben durch $\xi_b=Q^tb$.

Wir suchen immer noch einen Vektor $v$, so dass $Av-b$ im $Q$-Koordinatensystem
verschwindende $Q_{\|}$-Koordinaten hat.
In $Q$-Koordinaten hat dieser
Vektor die Koordinaten
\[
Q^t(Av-b)=Q^t(QRv-b)=Rv-Q^tb.
\]
Dass die $Q_{\|}$-Koordinaten dieses Vektors verschwinden sollen ist
gleichbedeutend damit, dass die ersten $n$ Komponenten von 
$Rv$ und $Q^tb$ übereinstimmen müssen.
Die ersten $n$ Komponenten
von $Q^tb$ sind aber $Q_{\|}^tb$, wir erhalten so ein Gleichungssystem
der Form
\begin{equation}
Rv=\begin{pmatrix}
Q_{\|}^tb\\
0
\end{pmatrix}.
\label{qr-leastsquares}
\end{equation}
Die letzten $m-n$ Gleichungen (mit $0$ auf der rechten Seite) sind dabei
wegen der Dreiecksform von $R$ automatisch erfüllt, man muss also nur
noch das $n\times n$-Gleichungssystem bestehend aus den ersten $n$
Gleichungen lösen.

Wir haben also wiederum nur eine $n\times n$-Gleichungssystem
zu lösen, wie beim Verfahren aus Abschnitt~\ref{section:ueberbestimmt}.
Trotzdem haben wir eine Vereinfachung erreicht:
\begin{itemize}
\item Wir müssen das Produkt $A^tA$ nicht mehr berechnen.
\item Es ist nur noch das Produkt $Q_{\|}^tb$ zu berechnen.
Da $Q_{\|}^t$
eine $n\times m$-Matrix ist, ist dies der gleiche Aufwand wie $A^tb$.
\item Das Gleichungssystem (\ref{qr-leastsquares}) ist bereits in Dreiecksform,
es kann also allein durch Rückwärtseinsetzen gelöst werden.
\item Falls es gelingt, einen effizienteren Algorithmus für die Bestimmung
der $QR$-Zerlegung zu finden, profitiert die Lösung des Least-Squares-Problems
davon.
\end{itemize}
Übrigens ist es nach der $QR$-Zerlegung sehr einfach, das Produkt $A^tA$
zu berechnen, denn es gilt:
\[
A^tA=(QR)^tQR=R^tQ^tQR=R^tR,
\]
und in der Matrix $R$ finden sich im Gegensatz zu $A$
nur in den ersten $n$ Zeilen von $0$ verschiedene Elemente.

