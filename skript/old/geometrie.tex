%
% Geometrie der linearen Gleichungssysteme
%
% (c) 2009 Prof Dr Andreas Mueller, Hochschule Rapperswil
%
\chapter{Lineare Räume\label{chapter-vr}}
\rhead{Lineare Räume}
Lineare Gleichungssysteme stellen nur das algebraische Hilfsmittel dar,
mit dem allgemeinere Probleme der linearen Algebra gelöst werden 
können.
Die Vektorgeometrie ist eine solche Anwendung, sie löst konkrete 
Probleme der dreidimensionalen Geometrie mit algebraischen Mitteln.
Die Beschränkung auf drei Dimensionen ist aber nur durch die unmittelbare
Anwendung gegeben, die daraus ableitbare Intuition kann durchaus 
auch in Räumen beliebig hoher Dimension als Leitlinie zur Lösung
abstrakter Probleme dienen.
Nachgerade kann es auch für dreidimensionale
Probleme von Vorteil sein, den dreidimensionalen Raum zu verlassen,
und das Problem in einem höherdimensionalen Raum zu lösen.

\index{Schnittgerade}
Als Beispiel betrachten wir das Problem, die Schnittgerade von
zwei Ebenen zu bestimmen.
Wir wollen illustrieren, dass die
Berechnung in einem 7-dimensionalen Raum schneller und
``computergerechter'' zu einer Lösung führt als die klassische
Methode, die zum Beispiel das Vektorprodukt verwendet.
Seien 
\begin{equation}
\begin{pmatrix}x\\y\\z\end{pmatrix}
=
\begin{pmatrix}6\\4\\7\end{pmatrix}
+t
\begin{pmatrix}3\\-2\\2\end{pmatrix}
+s
\begin{pmatrix}-5\\3\\-7\end{pmatrix}
,\qquad
\begin{pmatrix}x\\y\\z\end{pmatrix}
=
\begin{pmatrix}2\\2\\4\end{pmatrix}
+u
\begin{pmatrix}4\\11\\0\end{pmatrix}
+v
\begin{pmatrix}-1\\1\\3\end{pmatrix}
\label{zweiebenen}
\end{equation}
die Parameterdarstellungen der beiden Ebenen.
Gesucht ist die
Schnittgerade, auch wieder in Parameterdarstellung, also
zwei Vektoren $\vec p_0$ und $\vec r$ so, dass $\vec p_0+v\vec r$
für alle Werte von $v$ den Ortsvektor eines Punktes der
Schnittgeraden ergibt.
Die klassische Methode setzt diese beiden Vektorgleichungen
gleich, und findet somit ein Gleichungssystem mit vier unbekannten
$t$, $s$, $u$ und $v$.
Die Lösung erlaubt $u$ durch $v$ auszudrücken,
was man dann in die zweite Gleichung einsetzen muss, um die Parameterdarstellung
der Schnittgeraden zu finden.
Oder man verwendet das Vektorprodukt
um die Normalen $\vec n_1$ und $\vec n_2$ der beiden Ebenen zu finden, und daraus
erneut mit dem Vektorprodukt den Richtungsvektor
$\vec r=\vec n_1\times\vec n_2$ zu berechnen.
Dann braucht man nur noch einen
einzelnen Punkt der Schnittgeraden, den man zum Beispiel dadurch finden kann,
dass man $v=0$ setzt und damit das Gleichungssystem eindeutig lösbar
macht.

Es geht aber wesentlich einfacher in sieben Dimensionen.
Die Parameterdarstellungen (\ref{zweiebenen})
sind eigentlich 6 lineare Gleichungen für die sieben Unbekannten
$x$, $y$, $z$, $t$, $s$, $u$ und $v$, man kann (\ref{zweiebenen})
also auch als Gleichungssystem mit sechs Gleichungen für sieben
Unbekannte schreiben
\begin{equation}
\begin{linsys}{7}
x& & & & &-&3t&+&5s& &   & &  &=&6\\
 & &y& & &+&2t&-&3s& &   & &  &=&4\\
 & & & &z&-&2t&+&7s& &   & &  &=&7\\
x& & & & & &  & &  &-& 4u&+& v&=&2\\
 & &y& & & &  & &  &-&11u&-& v&=&2\\
 & & & &z& &  & &  & &   &-&3v&=&4\\
\end{linsys}
\label{sechsdimensional}
\end{equation}
Das gleiche System in Tableauform geschrieben ist
\begin{center}
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}>{$}c<{$}>{$}c<{$}>{$}c<{$}>{$}c<{$}|>{$}c<{$}|}
\hline
1&0&0&-3&5&0&0&6\\
0&1&0&2&-3&0&0&4\\
0&0&1&-2&7&0&0&7\\
1&0&0&0&0&-4&1&2\\
0&1&0&0&0&-11&-1&2\\
0&0&1&0&0&0&-3&4\\
\hline
\end{tabular}
\end{center}
Anwendung des Gauss-Algorithmus liefert
\begin{center}
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}>{$}c<{$}>{$}c<{$}>{$}c<{$}>{$}c<{$}|>{$}c<{$}|}
\hline
1&0&0&0&0&0& 1&\frac{10}3\\
0&1&0&0&0&0&-1&\frac{17}3\\
0&0&1&0&0&0&-3&4\\
0&0&0&1&0&0& 2&-\frac13\\
0&0&0&0&1&0& 1&\frac13\\
0&0&0&0&0&1& 0&\frac13\\
\hline
\end{tabular}
\end{center}
Schreibt man die ersten zwei Zeilen wieder als Gleichungen, wobei
man auch gleich $v$ auf die rechte Seite schafft, ergibt sich
\begin{equation}
\begin{linsys}{4}
x& & & & &=&\frac{10}3&-& v\\
 & &y& & &=&\frac{17}3&+& v\\
 & & & &z&=&4         &+&3v\\
\end{linsys}
\end{equation}
oder in Vektorform
\begin{equation}
\begin{pmatrix}x\\y\\z\end{pmatrix}
=\begin{pmatrix}\frac{10}3\\\frac{17}3\\4\end{pmatrix}
+v\begin{pmatrix}-1\\1\\-3\end{pmatrix},
\end{equation}
und damit die gesuchte Parameterdarstellung der Schnittgeraden.

Ziel dieses Kapitels ist daher, eine geometrische Sprache für die
Algebra in den Mengen $\mathbb R^n$ zu entwickeln und möglichst viel
der bekannten vektorgeometrischen Vorstellungen in diese Sprache
zu übertragen.

\section{Vektorräume \texorpdfstring{$\mathbb R^n$}{R hoch n} und Unterräume}
\index{Vektorraum@Vektorraum $\mathbb R^n$}
Die Menge $V$ der $n$-dimensionalen Spaltenvektoren bildet was man
einen Vektorraum nennt: Spaltenvektoren können addiert und mit
einem Skalar multipliziert werden.
Die beiden Operationen sind
miteinander verträglich, was durch die Distributivgesetze ausgedrückt
wird.
Intuitiv sagen diese, dass die ``übliche Algebra funktioniert'',
man kann mit diesen Vektoren genau so rechnen, wie man sich das von
der Algebra mit gewöhnlichen Zahlen gewohnt ist, wenigstens solange
man nicht versucht, Vektoren miteinander zu multiplizieren oder zu
dividieren.
Es gibt auch einen Nullvektor, der bei Addition nichts ändert,
also die Funktion der Null übernimmt.

Es gibt aber auch Teilmengen von $\mathbb R^n$, die vergleichbare
Eigenschaften haben.
Typischerweise können diese durch lineare Gleichungen definiert werden.
Die Menge
\[
V=\left\{\left.\begin{pmatrix}x\\y\end{pmatrix}\,\right|\,x=y\right\}
\subset\mathbb R^2
\]
besteht aus Vektoren der Form $\begin{pmatrix}x\\x\end{pmatrix}$,
es ist also ziemlich offensichtlich, dass die Summe und die skalaren
Vielfachen von  Vektoren aus $V$ wieder in $V$ liegen.
$V$ ist
bezüglich der Rechenoperationen abgeschlossen, und auch der Nullvektor
$0\in V$.

\index{Unterraum}
\begin{definition}
Eine Teilmenge $V\subset\mathbb R^n$ heisst Unterraum von $\mathbb R^n$,
wenn für zwei Vektoren
$u,v\in V$ und zwei reelle Zahlen $\lambda,\mu\in\mathbb R$
auch die daraus gebildete Linearkombination
$\lambda u+\mu v\in V$ ist.
\end{definition}

Der kleinste mögliche Unterraum ist $\{0\}$, also der Vektorraum, der nur aus
dem Nullvektor besteht.
Für diesen Raum schreibt man manchmal auch nur $0$.

\subsubsection{Beispiel: Nullraum}
\index{Nullraum}
\index{Kern|see{Nullraum}}
\index{ker|see{Nullraum}}
Sei $A$ eine $m\times n$-Matrix.
Die Menge
\[
U=\{v\in\mathbb R^n\,|\,Av=0\}
\]
ist ein Unterraum, der Nullraum oder Kern von $A$, geschrieben
$\operatorname{ker}A$.
Tatsächlich gilt für $u,v\in\operatorname{ker}A$
\[
A(\lambda v+\mu u)=\lambda Av+\mu Au=0,
\]
also ist $\lambda v+\mu u\in\operatorname{ker}A$.

\begin{satz} Sind $U$ und $V$ Unterräume, dann auch $U\cap V$ und
\[
U+V=\{u+v\,|\,u\in U,v\in V\}.
\]
\end{satz}

Sind $U$ und $V$ Unterräume von $\mathbb R^n$, dann ist auch
$U\cap V$ ein Unterraum.
Dazu ist zu prüfen, dass die Linearkombinationen
von zwei Vektoren $u,v\in U\cap V$ wieder in $U\cap V$ sind.
Da beide Vektoren aus $U$ sind, muss auch jede Linearkombination in $U$ sein.
Dasselbe gilt für $V$, also ist jede Linearkombination in $U\cap V$.

\begin{satz}
Sei $A$ eine $m\times n$-Matrix und $V$ ein Unterraum von $\mathbb R^n$.
Dann ist
\[
AV=\{ Av\,|v\in V\}\subset\mathbb R^m
\]
ein Unterraum von $\mathbb R^m$.
\end{satz}
\begin{proof}[Beweis]
Sind zwei Vektoren $u_1$ und $u_2$ in $AV$, dann gibt es $v_1,v_2\in V$ mit
$u_1=Av_1$ und $u_2=Av_2$.
Also ist
\[
\lambda_1u_1+\lambda_2u_2
=
\lambda_1Av_1+\lambda_2Av_2
=
A(\lambda_1v_1+\lambda_2v_2)\in AV,
\]
also ist $AV$ ein Unterraum.
\end{proof}
Falls $V=\mathbb R^n$ schreibt man auch $A\mathbb R^n=\operatorname{im}A$
und nennt $\operatorname{im}A$ das Bild von $A$.
\index{Bild}
\index{im@$\operatorname{im}A$|see{Bild}}

\section{Lösungen von linearen Gleichungen}
Die Sprache der linearen Räume erlaubt uns, die Erkenntnisse über
lineare Gleichungssysteme jetzt etwas geometrischer zu formulieren.

Sei $A$ eine $m\times n$-Matrix, $b$ ein $m$-dimensionaler Vektor und $x$
ein Vektor von $n$ Unbekannten.
Dann ist $Ax=b$ ein lineares Gleichungssystem
mit $m$ Gleichungen für $n$ Unbekannte.
Natürlich ist das Gleichungssystem
nur dann lösbar, wenn der Vektor $b$ im Bild der Matrix $A$ drin liegt.

Falls also $b\in\operatorname{im}A$ ist, gibt es auf jeden Fall mindestens
eine Lösung $x\in\mathbb R^n$ mit $Ax=b$.
Kann es auch mehrere geben? Nehmen
wir an, $x$ und $y$ seien beides solche Lösungen, dann ist
\[
\left.
\begin{aligned}
Ax&=b\\
Ay&=b
\end{aligned}
\right\}
\qquad\Rightarrow\qquad
Ax-Ay=A(x-y)=0
\qquad\Rightarrow\qquad
x-y\in\operatorname{ker}A.
\]
Der Nullraum von $A$ gibt also darüber Auskunft, ob ein Gleichungssystem
eine oder mehrere Lösungen hat:
\begin{satz}
Sei $A$ eine $m\times n$-Matrix und $b\in\mathbb R^m$.
Dann gilt:
\begin{enumerate}
\item Das Gleichungssystem hat keine Lösung, wenn $b\not\in \operatorname{im}A$.
\item Falls $b\in\operatorname{im}A$, und $x_p$ eine Lösung des Gleichungssystems
$Ax=b$ ist, dann ist jede andere Lösung von der Form $x=x_p+x_h$, wobei
$x_h\in\operatorname{ker}A$ ist.
\item Das Gleichungssystem hat genau dann nur eine Lösung, wenn $\operatorname{ker}A=0$
\end{enumerate}
\end{satz}
Das Gleichungssystem $Ax=0$ heisst das zu $Ax=b$ gehörige homogene System.
Der Satz sagt also, dass man eine einzige Lösung von $Ax=b$ finden muss,
dies ist das $x_p$, und die Lösungen von $Ax=0$, dass sind die Elemente
des Kerns von $A$, die im Satz mit $x_h$ bezeichnet werden.
Eine Lösung von $Ax=b$ ist dann immer von der Form $x_p+x_h$.
In der Tat bekommt
man 
\[
Ax=A(x_p+x_h)=Ax_p+Ax_h=b+0=b,
\]
also ist jedes solche $x$ tatsächlich eine Lösung.

Wie entscheidet man, ob $b\not\in\operatorname{im}A$? Natürlich mit
Hilfe des Gauss-Algorithmus, der ja klärt, ob das Gleichungssystem
$Ax=b$ eine Lösung hat.
Wie findet man die Vektoren in $ker A$?
Natürlich auch mit dem Gauss-Algorithmus, der ja auch die Lösungsmenge
von $Ax=0$ berechnen kann.

\section{Basis}
\index{Basis}
In den bisherigen Beispielen sind Vektorräume entweder als Nullräume oder
als Bildräume einer Matrix entstanden.
Der Test, ob ein Vektor in einem
Vektorraum drin ist, muss entsprechend auf verschiedene Art erfolgen:
\begin{enumerate}
\item Ist $V=\operatorname{ker}A$, berechnet man einfach $Av$, falls $Av=0$
kann man schliessen, dass $v\in V$.
\item Ist $V=\operatorname{im}A$, dann muss man einen Vektor $u\in\mathbb R^n$
finden mit $v=Au$, man muss also ein lineares Gleichungssystem lösen.
\end{enumerate}
Der zweite Fall ist also deutlich aufwendiger.
Schreibt man die Bedingung
$v=Au$ als Gleichung von Spaltenvektoren von $A$, bekommt man
\[
\begin{pmatrix}v_1\\\vdots\\v_m\end{pmatrix}
=
\begin{pmatrix}a_{11}\\\vdots\\a_{m1}\end{pmatrix}u_1+\dots+
\begin{pmatrix}a_{1n}\\\vdots\\a_{mn}\end{pmatrix}u_n.
\]
Man muss also entscheiden, ob sich $v$ durch die Spaltenvektoren von
$A$ ausdrücken lässt.
Selbst wenn $\operatorname{im}A$ nur ein
relativ kleiner Raum ist, zum Beispiel weil alle Spalten von $A$
Vielfache eines
einzigen Vektors sind, gibt dies ziemlich viel Arbeit.
Wir möchten
daher einen Vektorraum als Bildraum einer möglichst kleinen Matrix
schreiben können.
Gleichbedeutend damit ist, dass wir die Vektoren
des Raumes aus einer möglichst kleinen Zahl von Vektoren linear
kombinieren können möchten.

Diese Art von Darstellung beliebiger Vektoren mit Hilfe einer
ausgewählten Familie von Vektoren liegt dem Koordinaten-Systemen
zu Grunde.
Um die Koordinaten eines Punktes zu finden, zerlegt
man seinen Ortsvektor in die Vektoren entlang der Koordinatenachsen:
\[
\begin{pmatrix}2\\3\\5\end{pmatrix}
=
2\begin{pmatrix}1\\0\\0\end{pmatrix}+
3\begin{pmatrix}0\\1\\0\end{pmatrix}+
5\begin{pmatrix}0\\0\\1\end{pmatrix}
\]
Es müssen aber zwei Dinge sichergestellt werden:
\begin{enumerate}
\item Jeder Vektor muss in dieser Form darstellbar sein.
Für die
Standardbasisvektoren ist dies offensichtlich, aber wie findet man
dies bei einer beliebigen Menge von Vektoren heraus?
\item Die Darstellung muss eindeutig sein, sonst hat man offensichtlich
mehrere Beschreibungen des gleichen Punktes.
Wie kann man entscheiden,
ob ein Vektor nur auf eine Art aus den Vektoren kombiniert werden kann?
\end{enumerate}
Die nächsten zwei Abschnitte adressieren diese Fragen.

\subsection{Aufgespannter Raum}
\index{aufspannter Raum}
Wir müssen also zunächst klären, was es heisst, dass wir ``genügend''
Vektoren haben, um den Unterraum $V$ zu bilden.

\begin{definition}
\index{erzeugen}
Eine Menge $B=\{b_1,b_2,\dots,b_n\}$ von Vektoren erzeugt den
linearen Raum
\[
\langle B\rangle =
\langle b_1,\dots , b_n\rangle =
\{\lambda_1b_1+\dots+\lambda_nb_n\,|\,\lambda_i\in\mathbb R\}.
\]
Man sagt auch, $B$ spannt den Raum $\langle B\rangle$ auf.
\end{definition}

\begin{beispiel}[Beispiel: Standardbasisvektoren]
\index{Standardbasisvektoren}
Die speziellen Vektoren
\[
e_1=\begin{pmatrix}1\\0\\\vdots\\0\end{pmatrix},\quad
e_2=\begin{pmatrix}0\\1\\\vdots\\0\end{pmatrix},
\dots,
e_n=\begin{pmatrix}0\\0\\\vdots\\1\end{pmatrix}
\]
erzeugen den ganzen Raum $\mathbb R^n$, denn jeder Vektor $v\in\mathbb R^n$
lässt sich schreiben als
\[
v=\begin{pmatrix}v_1\\v_2\\\vdots\\v_n\end{pmatrix}
=
v_1\begin{pmatrix}1\\0\\\vdots\\0\end{pmatrix}+
v_2\begin{pmatrix}0\\1\\\vdots\\0\end{pmatrix}+
\dots+
v_n\begin{pmatrix}0\\0\\\vdots\\1\end{pmatrix}.
\]
Somit ist $\mathbb R^n=\langle e_1,e_2,\dots,e_n\rangle$.
die Vektoren $e_i$ heissen die Standardbasisvektoren.
\end{beispiel}

\begin{beispiel}
Gegeben seien die Vektoren
\[
a_1=\begin{pmatrix}1\\4\\7\end{pmatrix},\quad
a_2=\begin{pmatrix}2\\5\\8\end{pmatrix},\quad
a_3=\begin{pmatrix}3\\6\\9\end{pmatrix}
\]
Spannen diese drei Vektoren den ganzen Raum $\mathbb R^3$ auf,
oder anders ausgedrückt, kann man damit jeden Vektor in $\mathbb R^3$
linear kombinieren?

Dies wäre möglich, wenn die offensichtlich ganz $\mathbb R^3$
aufspannenden Vektoren $e_1,e_2,e_3$ durch $a_i$ ausgedrückt werden
können.
Also testen wir, ob $e_i\in \operatorname{im}A$, wobei
\[
A=\begin{pmatrix}
1&2&3\\
4&5&6\\
7&8&9
\end{pmatrix}
\]
ist.
Der Gauss-Algorithmus gibt folgende Tableaus:
\begin{align*}
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}|>{$}c<{$}>{$}c<{$}>{$}c<{$}|}
\hline
1%
\begin{picture}(0,0)
\color{red}\put(-3,4){\circle{12}}
\end{picture}%
&2&3&1&0&0\\
4&5&6&0&1&0\\
7%
\begin{picture}(0,0)
\color{blue}\drawline(-8,-2)(-8,25)(1,25)(1,-2)
\end{picture}%
&8&9&0&0&1\\
\hline
\end{tabular}
&\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}|>{$}c<{$}>{$}c<{$}>{$}c<{$}|}
\hline
1&2&3&1&0&0\\
0&-3%
\begin{picture}(0,0)
\color{red}\put(-7,4){\circle{15}}
\end{picture}%
&-6&-4&1&0\\
0&-6%
\begin{picture}(0,0)
\color{blue}\drawline(-15,-2)(-15,10)(1,10)(1,-2)
\end{picture}%
&-12&-7&0&1\\
\hline
\end{tabular}
\\
&\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}|>{$}c<{$}>{$}c<{$}>{$}c<{$}|}
\hline
1&2&3&1&0&0\\
0&1&2&\frac43&-\frac13&0\\
0&0&0&1&-2&1\\
\hline
\end{tabular}
\end{align*}
Man kann also keinen einzigen der Vektoren $e_i$ mit den Vektoren $a_i$
darstellen, die Vektoren $a_i$ können also unmöglich den ganzen
$\mathbb R^3$ aufspannen.
\end{beispiel}

\subsection{Basis}
\index{Basis}
Eine Menge von Vektoren, mit denen man alle Vektoren eines Unterraums
linear kombinieren kann, muss linear unabhängig sein.
Sonst könnte
man nämlich einen der Vektoren durch die anderen ausdrücken, damit
wird er selbst unnötig.

\begin{definition}
\index{Basis}
Ein Menge $B\subset V$ von Vektoren in einem linearen Raum $V$ heisst
Basis, wenn gilt:
\begin{enumerate}
\item $B$ spannt $V$ auf: $V=\langle B\rangle$.
\item Die Vektoren in $B$ sind linear unabhängig.
\end{enumerate}
\end{definition}

\begin{beispiel}
Der Raum
\[
\left\{\left.\begin{pmatrix}x\\y\end{pmatrix}\,\right|\, x=y\right\}
\]
wird aufgespannt von 
\[
B=\left\{\begin{pmatrix}1\\1\end{pmatrix}\right\}.
\]
\end{beispiel}

Wie kann man testen, ob eine Menge von Vektoren eine Basis ist?
Schreibt man die Vektoren der Basis $B$ als Spalten in eine Matrix
$\tilde B$, müssen wir offenbar zwei Dinge testen:
\begin{enumerate}
\item $V = \operatorname{im}\tilde B$: das Gleichungssystem $\tilde Bx=b$
muss für jeden Vektor $b\in V$ lösbar sein.
\item Die Vektoren von $B$ sind linear unabhängig: die Spalten sind
linear unabhängig.
Dies kann auf zwei Arten geschehen:
\begin{enumerate}
\item Der Gauss-Algorithmus, angewendet auf $\tilde B^t$, liefert eine
Nullzeile genau dann, wenn die Zeilen von $\tilde B^t$ linear abhängig sind,
das sind aber genau die Spalten von $\tilde B$.
Wenn also keine Nullzeile
auftritt, sind die Vektoren linear unabhängig.
\item Falls der Gauss-Algorithmus für das homogene Gleichungssystem
$\tilde B x=0$ nur die Lösung $x=0$ findet, sind die Spalten linear
unabhängig.
Anders ausgedrückt: wenn das Gleichungssystem $\tilde Bx=b$
für jedes in Frage kommende $b$ genau eine Lösung hat, sind die
Spalten von $\tilde B$ linear unabhängig.
\end{enumerate}
\end{enumerate}


\begin{satz}
\index{Standardbasis}
Die Standardbasisvektoren bilden eine Basis von $\mathbb R^n$,
die Standardbasis.
\end{satz}

\begin{proof}[Beweis]
Wir wissen bereits, dass die Standardbasisvektoren $\mathbb R^n$ aufspannen,
wir müssen aber nur noch erkennen, dass sie auch linear unabhängig sind.
Schreibt man die Vektoren als Spalten in eine Matrix $\tilde B$, entsteht die
Einheitsmatrix, und ein Gleichungssystem $Ex=b$ hat immer genau die eine
Lösung $x=b$, also ist 
\end{proof}

\begin{beispiel}
Die Vektoren 
\[
b_1=\begin{pmatrix}1\\0\\0\end{pmatrix},\quad
b_2=\begin{pmatrix}1\\1\\0\end{pmatrix},\quad
b_3=\begin{pmatrix}1\\1\\1\end{pmatrix},\quad
\]
bilden eine Basis von $\mathbb R^3$.
Zwei Dinge sind zu prüfen:
spannen sie den ganzen Raum auf und sind sie linear unabhängig.
Da man alle Standardbasisvektoren durch die $b_i$ ausdrücken kann, nämlich
durch
\[
e_1=b_1,\qquad e_2=b_2-b_1,\qquad\text{und}\qquad e_3=b_3-b_2-b_1,
\]
ist jeder Vektor durch die $b_i$ ausdrückbar.
Lineare Abhängigkeit kann
man mit dem Gauss-Algorithmus testen.
Wir schreiben dazu die Vektoren
als Zeilen in ein Gauss-Tableau:
\[
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}|}
\hline
1&0&0\\
1&1&0\\
1&1&1\\
\hline
\end{tabular}
\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}|}
\hline
1&0&0\\
0&1&0\\
0&0&1\\
\hline
\end{tabular}
\]
Da keine Nullzeile entstanden ist, sind die Zeilen linear unabhängig.
Die Zeilen waren aber genau die Vektoren $b_i$.
Damit ist klar, dass
die Vektoren $b_i$ eine Basis bilden.
\end{beispiel}

\begin{definition}
\index{Dimension}
Die Dimension $\dim V$ eines Vektorraumes $V$ ist die Zahl der
Basisvektoren einer Basis von $V$.
\end{definition}

\begin{beispiel} Man finde eine Basis des Nullraumes der Matrix
\[
A=\begin{pmatrix}
1&2&3\\
4&5&6\\
7&8&9
\end{pmatrix}.
\]
Wendet man den Gauss-Algorithmus für das homogene Gleichungssystem
an, findet man:
\begin{align*}
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}|>{$}c<{$}|}
\hline
 1%
\begin{picture}(0,0)
\color{red}\put(-3,4){\circle{12}}
\end{picture}%
& 2& 3&0\\
 4& 5& 6&0\\
 7%
\begin{picture}(0,0)
\color{blue}\drawline(-8,-2)(-8,24)(1,24)(1,-2)
\end{picture}%
& 8& 9&0\\
\hline
\end{tabular}
&\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}|>{$}c<{$}|}
\hline
 1& 2&  3&0\\
 0&-3%
\begin{picture}(0,0)
\color{red}\put(-6,4){\circle{15}}
\end{picture}%
& -6&0\\
 0&-6%
\begin{picture}(0,0)
\color{blue}\drawline(-15,-2)(-15,10)(2,10)(2,-2)
\end{picture}%
&-12&0\\
\hline
\end{tabular}
\\
&\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}|>{$}c<{$}|}
\hline
 1& 2%
\begin{picture}(0,0)
\color{blue}\drawline(-8,10)(-8,-2)(2,-2)(2,10)
\end{picture}%
&  3&0\\
 0& 1&  2&0\\
 0& 0&  0&0\\
\hline
\end{tabular}
\\
&\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}|>{$}c<{$}|}
\hline
 1& 0& -1&0\\
 0& 1&  2&0\\
 0& 0&  0&0\\
\hline
\end{tabular}
\end{align*}
Offenbar gibt es genau eine frei wählbare Variable $z$, und
die Lösungsmenge ist 
\[
t\begin{pmatrix}
1\\-2\\1
\end{pmatrix}
\]
Der Nullraum wird also vom Vektor
\[
\begin{pmatrix}
1\\-2\\1
\end{pmatrix}
\]
aufgespannt.
\end{beispiel}

\begin{beispiel}
Man finde eine Basis des Bildraumes $\operatorname{im}A$ mit der gleichen
Matrix wie im vorangegangenen Beispiel.

Die Basis besteht aus so wenigen linear unabhängigen Vektoren wie
möglich, aber alle Spaltenvektoren von $A$ müssen damit erzeugt
werden können.
Dazu nehmen wir einfach einen Vektor um den anderen
hinzu, solange die Menge der Vektoren linear unabhängig bleibt.

Der erste Spaltenvektor ist nicht der Nullvektor, also können wir
den in die Basis hinein nehmen.
Der zweite ist nicht proportional,
also sind die ersten beiden Spalten linear unabhängig.
Als
Basis könnten wir daher
\[
\left\{
\begin{pmatrix}1\\4\\7\end{pmatrix}
,
\begin{pmatrix}2\\5\\8\end{pmatrix}
\right\}
\]
nehmen.
Den dritten Spaltenvektor dürfen wir nicht hinzunehmen.
Aus dem letzten Beispiel wissen wir ja, dass die Matrix singulär
ist.
Es kann also höchstens zwei linear unabhängige Vektoren haben.
\end{beispiel}

\subsection{Koordinaten}
\index{Koordinaten}
Gibt man eine Basis $B=\{b_1,\dots,b_k\}$ von $V$ vor,
dann kann man die Vektoren
in $V$ in der Basis $B$ ausdrücken.
Dazu muss man zu einem 
Vektor $v\in V$ Zahlen $\xi_i$ finden mit
\[
v=\xi_1 b_1+\dots +\xi_k b_k.
\]
Schreiben wir wieder die Basisvektoren von $B$ als Spaltenvektoren in 
eine Matrix $\tilde B$, dann ist 
\[
v=\tilde B\begin{pmatrix}\xi_1\\\vdots\\\xi_n\end{pmatrix}.
\]
Eine Basis eines $k$-dimensionalen Raumes $V$ ermöglicht also,
die Vektoren von $V$ mit Hilfe von $k$-Tupeln, bestehend
aus den Zahlen $\xi_i$, darzustellen.
Die $\xi_i$ heissen
Koordinaten eines Vektors $v$ in der Basis $B$, wir schreiben
oft auch einfach $\xi$ für den Vektor mit Komponenten $\xi_i$.
Das Finden der Koordinaten eines Vektors $v$ läuft immer auf die Lösung
des Gleichungssystems $\tilde B\xi=v$ hinaus.

\begin{beispiel}
Die Basis \[
B=\left\{
\begin{pmatrix}1\\2\\3\end{pmatrix},
\begin{pmatrix}3\\2\\1\end{pmatrix}
\right\}
\]
spannt einen zweidimensionalen Unterraum von $\mathbb R^3$ auf.
Man finde
die Koordinaten des Vektors 
\[
v=
\begin{pmatrix}
0\\4\\8
\end{pmatrix}
\]
in der Basis $B$.

Dazu muss man das Gleichungssystem $v=\tilde B\xi$ lösen:
\[
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|>{$}c<{$}|}
\hline
1%
\begin{picture}(0,0)
\color{red}\put(-3,4){\circle{12}}
\end{picture}%
&3&0\\
2&2&4\\
3%
\begin{picture}(0,0)
\color{blue}\drawline(-8,-2)(-8,25)(2,25)(2,-2)
\end{picture}%
&1&8\\
\hline
\end{tabular}
\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|>{$}c<{$}|}
\hline
1&3&0\\
0&-4%
\begin{picture}(0,0)
\color{red}\put(-7,4){\circle{15}}
\end{picture}%
&4\\
0&-8%
\begin{picture}(0,0)
\color{blue}\drawline(-15,-2)(-15,10)(1,10)(1,-2)
\end{picture}%
&8\\
\hline
\end{tabular}
\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|>{$}c<{$}|}
\hline
1&3%
\begin{picture}(0,0)
\color{blue}\drawline(-8,10)(-8,-2)(1,-2)(1,10)
\end{picture}%
&0\\
0&1&-1\\
0&0&0\\
\hline
\end{tabular}
\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|>{$}c<{$}|}
\hline
1&0&3\\
0&1&-1\\
0&0&0\\
\hline
\end{tabular}
\]
man kann also die Koordinaten $3$ und $-1$ ablesen.
Kontrolle:
\[
\tilde B\xi
=
\begin{pmatrix}
1&3\\
2&2\\
3&1\end{pmatrix}
\begin{pmatrix}3\\-1\end{pmatrix}
=\begin{pmatrix}
0\\4\\8
\end{pmatrix}
=v.
\]
\end{beispiel}

\subsection{Basiswechsel}
\index{Basiswechsel}
Jede beliebige linear unabhängige Teilmenge von $V$, welche ganz $V$
aufspannt, kann als Basis verwendet werden.
Oft ist es praktischer,
statt der Standardbasis eine andere Basis zu verwenden, zum Beispiel
um die Koordinatenachsen parallel zu den Kanten eines Werkstücks zu
bekommen, oder um eine spezielle Symmetrie des Problems einfacher
ausdrücken zu können.
Wie sind die Koordinaten zwischen zwei Basen $B$ und $B'$ umzurechnen?

Zu jeder Basis $B$ gibt es die Matrix $\tilde B$, die aus den
Koordinaten $\xi$ eines Vektors $v$ den Vektor mittels $v=\tilde B\xi$
berechnet.
Hat man zwei Basen $B$ und $B'$, hat auch jeder Vektor
in $V$ zwei verschiedene Koordinaten-Vektoren $\xi$ und $\xi'$:
\[
v=\tilde B\xi =\tilde B'\xi'.
\]
Wie kann man $\xi'$ aus $\xi$ berechnen?

\subsubsection{Spezialfall $\mathbb R^n$}
In diesem Fall sind die beiden Matrizen $\tilde B$ und $\tilde B'$
reguläre $n\times n$-Matrizen.
Die Gleichung
\[
v=\tilde B\xi =\tilde B'\xi'.
\]
Kann dann durch Multiplikation mit $\tilde B^{-1}$ oder $\tilde B^{\prime-1}$
von links aufgelöst werden:
\begin{align*}
\tilde B^{\prime-1}\tilde B\xi&= \tilde B^{\prime -1}\tilde B'\xi'=E\xi'=\xi'\\
\xi=\tilde B^{-1}B\tilde B\xi&=\tilde B^{-1}\tilde B'\xi'
\end{align*}
Daraus können wir die Matrix zur Berechnung von $\xi'$ aus $\xi$
ablesen: 
\[
T=\tilde B^{\prime-1}B \quad\Rightarrow\quad
\xi'=T\xi.
\]

\subsubsection{Allgemeiner Fall}
Im allgemeinen Fall eines beliebigen Unterraumes funktioniert diese
Methode nicht.
$\tilde B$ und $\tilde B'$ sind keine quadratischen
Matrizen, also können Sie auch nicht invertiert werden.
Vielmehr sind sie jetzt $n\times m$-Matrizen mit $m<n$.

Wenn $B$ und $B'$ Basen sind, dann lässt sich jeder Vektor $v\in
\operatorname{im}\tilde B=\operatorname{im}\tilde B'$ 
sowohl mit $\xi$-Koordinaten als auch mit $\xi'$-Koordinaten
beschreiben:
\[
v=\tilde B\xi=\tilde B'\xi'.
\]
Darin sind die $\xi$- und $\xi'$-Vektoren sind Vektoren in $\mathbb R^m$.

Speziell könnte man für die $\xi$-Vektoren die Standardbasisvektoren
in $R^m$ wählen.
Mit dem ersten Standardbasisvektor bekäme man
das Gleichungssystem
\begin{equation}
\tilde B\begin{pmatrix} 1\\0\\\vdots\\0\end{pmatrix}
=\tilde B'\xi'
\label{t-gleichung}
\end{equation}
Dieses Gleichungssystem kann man mit dem Gauss-Algorithmus auflösen,
und $\xi'$ finden.
Dasselbe kann man natürlich auch für den
zweiten Standardbasis-Vektor machen, und auch für alle folgenden.

Wir können alle diese Gleichungen zusammen in eine einzige
Matrixgleichung schreiben:
\[
\tilde B\begin{pmatrix}
1&0&\dots&0\\
0&1&\dots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\dots&1
\end{pmatrix}
=B'\begin{pmatrix}
\mathstrut t_{11}&t_{12}&\dots&t_{1m}\\
\mathstrut t_{21}&t_{22}&\dots&t_{2m}\\
\mathstrut \vdots&\vdots&\ddots&\vdots\\
\mathstrut t_{m1}&t_{m2}&\dots&t_{mm}\\
\end{pmatrix}
\]
Die Matrix $T$ auf der rechten Seite enthält als Spalten
die Lösungen der Gleichungen (\ref{t-gleichung}).

Da die Spalten von $T$ Lösungen eines linearen Gleichungssystems
mit Koeffizienten $\tilde B'$ und rechten Seiten $B$ ist, können
wir das Verfahren zur simultanen Lösung anwenden.
Das zugehörige Gauss-Tableau ist:
\[
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}|>{$}c<{$}>{$}c<{$}>{$}c<{$}|}
\hline
\quad\mathstrut&         &\quad\mathstrut&\quad\mathstrut&        &\quad\mathstrut\\
      &         &      &      &        &      \mathstrut\\
      &         &      &      &        &      \mathstrut\\
      &\tilde B'&      &      &\tilde B&      \mathstrut\\
      &         &      &      &        &      \mathstrut\\
      &         &      &      &        &      \mathstrut\\
      &         &      &      &        &      \mathstrut\\
\hline
\end{tabular}
\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}>{$}c<{$}>{$}c<{$}|>{$}c<{$}>{$}c<{$}>{$}c<{$}|}
\hline
1     &     0&\dots   &0      & &                   & \\
0     &     1&\dots   &0      & &                   & \\
\vdots&\vdots&\ddots  &\vdots & &\raisebox{7pt}{$T$}& \\
0     &     0&\dots   &1      & &                   & \\
\hline
0     &     0&\dots   &0      &*&                   &*\\
0     &     0&\dots   &0      &*&                   &*\\
\hline
\end{tabular}
\]
Weil wir wissen, dass die Gleichungen immer eine Lösung haben, finden
wir an den Plätzen der Sterne unten rechts immer $0$, also steht im
Feld oben rechts genau die Transformationsmatrix $T$.

\begin{beispiel}
In der Ebene aufgespannt von den Vektoren 
\[
b_1=\begin{pmatrix}1\\1\\0 \end{pmatrix}
,\qquad
b_2=\begin{pmatrix}0\\1\\1 \end{pmatrix}
\]
möchte man die Basis aus den Basisvektoren
\[
b_1'=\begin{pmatrix}1\\2\\1\end{pmatrix}
,\qquad
b_2'=\begin{pmatrix}1\\0\\-1\end{pmatrix}
\]
Finden Sie die Koordinatentransformationsmatrix $T$, mit der
man von den Parameter $t$ $s$ in der Parameterdarstellung
mit Richtungsvektoren $b_1$ und $b_2$ auf die Koordinaten in
der Basis $B'$ umrechnen kann.
Man finde ausserdem die 
Koordinaten in der Basis $B'$ des Vektors, der in der Basis $B$
die Koordinaten $(2,-1)$ hat.

\begin{align*}
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|>{$}c<{$}>{$}c<{$}|}
\hline
1& 1&1&0\\
2& 0&1&1\\
1&-1&0&1\\
\hline
\end{tabular}
&\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|>{$}c<{$}>{$}c<{$}|}
\hline
1& 1& 1&0\\
0&-2&-1&1\\
0&-2&-1&1\\
\hline
\end{tabular}
\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|>{$}c<{$}>{$}c<{$}|}
\hline
1& 1&      1&       0\\
0& 1&\frac12&-\frac12\\
\hline
0& 0&      0&       0\\
\hline
\end{tabular}
\\
&\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|>{$}c<{$}>{$}c<{$}|}
\hline
1& 0& \frac12& \frac12\\
0& 1& \frac12&-\frac12\\
\hline
0& 0&      0&       0\\
\hline
\end{tabular}
\end{align*}
Die Transformationsmatrix ist also 
\[
T=
\frac12\begin{pmatrix} 1&1\\1&-1 \end{pmatrix}.
\]
Die Koordinaten $(2,-1)$ ergeben nach Umrechnung mit $T$ 
\[
\xi'=
T\begin{pmatrix}2\\-1\end{pmatrix}
=
\frac12\begin{pmatrix} 1&1\\1&-1 \end{pmatrix}
\begin{pmatrix}2\\-1\end{pmatrix}
=\frac12\begin{pmatrix}1\\3\end{pmatrix}.
\]
Zur Kontrolle berechnen wir den zugehörigen Vektor in $\mathbb R^3$:
\[
\tilde B\xi=
\begin{pmatrix}
1&0\\
1&1\\
0&1
\end{pmatrix}
\begin{pmatrix}2\\-1\end{pmatrix}
=\begin{pmatrix} 2\\1\\-1 \end{pmatrix}
,\qquad
\tilde B' \xi'
=
\begin{pmatrix}
1& 1\\
2& 0\\
1&-1
\end{pmatrix}\begin{pmatrix}\frac12\\\frac32\end{pmatrix}
=\begin{pmatrix}2\\1\\-1 \end{pmatrix},
\]
beide stimmen überein.
\end{beispiel}

\section{Lineare Abbildungen}
\subsection{Definition}
Eine $m\times n$-Matrix $A$ berechnet aus einem gegeben Vektor
$v\in\mathbb R^n$ mit Hilfe des
Matrizenproduktes einen neuen Vektor $Av\in\mathbb R^m$.
$A$ definiert also auch eine Abbildung
\[
A\colon\mathbb R^n\to\mathbb R^m:v\mapsto Av
\]
mit den Eigenschaften
\[
\left.
\begin{aligned}
A(u+v)&=Au+Av\\
A(\lambda u)&=\lambda Au
\end{aligned}\right\}\quad
\Rightarrow\quad
A(\lambda u+\mu v)=\lambda Au+\mu Av.
\]
Eine solche Abbildung heisst linear.
\index{lineare Abbildung}

\subsection{Matrix einer linearen Abbildung}
Umgekehrt definiert jede lineare Abbildung $V\to V$ mit Hilfe einer
Basis auch eine Matrix.
Sei $\varphi\colon V\to V$ eine Abbildung
mit $\varphi(\lambda u+\mu v)=\lambda \varphi(u)+\mu\varphi(v)$,
und $B=\{b_1,\dots,b_n\}$ eine Basis.
Dann genügt die Kenntnis
von $\varphi(b_i)$ für alle $i$, um die lineare Abbildung für
jeden beliebigen Vektor berechnen zu können.

Man kann nämlich jeden Vektor mit Hilfe seiner Koordinanten aus
den Basisvektoren linear kombinieren, also gilt auch
\[
\varphi(v)
=
\varphi(\xi_1b_1+\dots+\xi_nb_n)
=
\xi_1\varphi(b_1)+\dots+\xi_n\varphi(b_n).
\]
Ausserdem kann man die Koordinaten von $\varphi(b_i)$ in der Basis
$B$ ermitteln, und diese Koordinatenvektoren in eine Matrix $A$
schreiben.
Dann ist $A\xi$ der Koordinatenvektor von $\varphi(v)$.
Die Matrix $A$ enthält also in den Spalten die Bilder der Basisvektoren.

\subsection{Basiswechsel}
Wie sieht die Matrix einer linearen Abbildung aus, wenn man statt
der Basis $B$ die Basis $B'$ verwendet? Sei $T$ die Transformationsmatrix,
die Koordinaten bezüglich $B$ in Koordinaten bezüglich $B'$ umrechnet.
Die lineare Abbildung hat bezüglich der Basis $B$ die Matrix $A$, wir
suchen aber die Matrix $A'$, die die lineare Abbildung bezüglich der
Basis $B'$ beschreibt:
\[
\xymatrix{
\mathbb R^n\ar[r]^{A} \ar[d]_{T}
	&\mathbb R^n \ar[d]^{T}
\\
\mathbb R^n\ar[r]^{A'}
	&\mathbb R^n
}
\]
Aus dem Diagramm können wir ablesen, dass $A'$ gleichbedeutend
ist damit, die Koordinaten zuerst mit $T^{-1}$ von der Basis $B'$
auf die Basis $B$ umzurechnen, dort die Matrix $A$ anzuwenden, und
dann erneut mit $T$ in die Basis $B'$ zurückzukehren:
\begin{equation}
A'=TAT^{-1}.
\label{abbildung-basiswechsel}
\end{equation}

\section{Skalarprodukt}
\index{Skalarprodukt}
\subsection{Skalarprodukte und symmetrische Matrizen}
Das in der Vektorgeometrie definiert Skalarprodukt kann in beliebig
vielen Dimensionen definiert werden.
Zwei Vektoren $v$ und $u$ haben
das Skalarprodukt
\[
u\cdot v=
\begin{pmatrix}u_1\\\vdots\\u_n\end{pmatrix}
\cdot
\begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}
=u_1v_1+\dots+u_nv_n=u^tv.
\]
In den Anwendungen findet man aber weitere Beispiel von
Skalarprodukt-ähnlichen Grössen, zum Beispiel das Trägheitsmoment.
Die Rotationsenergie eines starren Körpers kann aus dem Vektor $\omega$
der Winkelgeschwindigkeit mit der (symmetrischen) Matrix des
Trägheitsmomentes $\Theta$
berechnet werden:
$
E=\frac12 \omega^t\Theta \omega.
$
Die allgemeinste Form eines Skalarproduktes verwendet daher eine
symmetrische Matrix $M$, also eine Matrix mit der Eigenschaft
$M=M^t$, und berechnet das Skalarprodukt zweier Vektoren $u$ und $v$
als $u^tMv$.

Mit dem Skalarprodukt kann auch die Länge eines Vektors definiert 
werden, wir schreiben
\[
|v|=\sqrt{v^tv}
\]
für die Länge.

\subsection{Basiswechsel}
Sei wieder $T$ die Matrix, die Koordinaten von der Basis $B'$ in die
Basis $B$ umrechnet.
Um das Skalarprodukt bezüglich der Basis $B'$ zu berechnen,
müssen die Vektoren zuerst mit $T$ in die Basis $B$ umgerechnet werden,
so dass dann die Formel für das Skalarprodukt in der Basis $B$
verwendet werden kann:
\[
(T\xi)^tT\eta=\xi^tT^tT\eta
\]
Die Matrix $T^tT$ ist symmetrisch, der ``Spezialfall'' eines Skalarproduktes,
welches mit Hilfe einer symmetrischen Matrix definiert worden ist,
ist also unvermeidlich, wenn man die Basis wechseln will.

\subsection{Orthogonale Matrizen}
\index{Matrix!orthognale}
\subsubsection{Definition}
Das Skalarprodukt erlaubt, Längen und Winkel zu berechnen.
Lineare Abbildungen, welche Längen und Winkel nicht ändern, beschreiben
Bewegungen des Raumes.
Damit zwei Vektoren $u$ und $v$ sollen nach Abbildung
mit der Matrix $A$ immer noch das gleiche Skalarprodukt haben, muss
\[
u^tv=(Au)^tAv=u^tA^tAv
\]
gelten.
Dies ist jedoch nur dann für alle Vektoren $u$ und $v$ möglich,
wenn $A^tA$ die Einheitsmatrix ist, also $A^tA=E$.
Das ist aber gleichbedeutend damit, dass $A^t=A^{-1}$.
Lineare Abbildungen, die Längen und
Winkel erhalten, haben also besonders einfach zu invertieren Matrizen.

\begin{definition}
\index{orthogonal}
Eine Matrix $A$ heisst orthogonal, wenn $A^tA=AA^t=E$.
\end{definition}

\begin{beispiel}
Die Matrix 
\[
A=
\begin{pmatrix}
\frac{\sqrt{2}}2& \frac{\sqrt{2}}2\\
-\frac{\sqrt{2}}2& \frac{\sqrt{2}}2
\end{pmatrix}
\]
ist orthogonal:
\[
A^tA=
\begin{pmatrix}
\frac{\sqrt{2}}2&-\frac{\sqrt{2}}2\\
\frac{\sqrt{2}}2& \frac{\sqrt{2}}2
\end{pmatrix}
\begin{pmatrix}
\frac{\sqrt{2}}2& \frac{\sqrt{2}}2\\
-\frac{\sqrt{2}}2& \frac{\sqrt{2}}2
\end{pmatrix}
=
\begin{pmatrix}
\frac24+\frac24&\frac24-\frac12\\
\frac24-\frac24&\frac24+\frac12
\end{pmatrix}
=E.
\]
\end{beispiel}

\subsubsection{Drehungen im zweidimensionalen Raum}
Die Matrix
\[
D_\alpha
=
\begin{pmatrix}
\cos\alpha&-\sin\alpha\\
\sin\alpha& \cos\alpha
\end{pmatrix}
\]
ist orthogonal:
\[
D_\alpha^tD_\alpha
=
\begin{pmatrix}
 \cos\alpha&\sin\alpha\\
-\sin\alpha&\cos\alpha
\end{pmatrix}
\begin{pmatrix}
\cos\alpha&-\sin\alpha\\
\sin\alpha& \cos\alpha
\end{pmatrix}
=
\begin{pmatrix}
\cos^2\alpha+\sin^2\alpha&0\\
0&\sin^2\alpha+\cos^2\alpha
\end{pmatrix}
=E.
\]
Diese Matrix beschreibt eine Drehung um den Winkel $\alpha$
in zwei Dimensionen.

\subsubsection{Spiegelungen}
\index{Spiegelung}
Die Matrix einer Spiegelung an einer Geraden ist orthogonal.

\smallskip

{\parindent 0pt
Sei} $v$ ein zweidimensionaler Vektor der Länge $1$.
Dann ist die Abbildung 
\[
u\mapsto u-2v(v\cdot u)
\]
die Spiegelung an der Geraden mit der Normalen $u$.
Ist nämlich
ein Vektor $u\perp v$, ist das Skalarprodukt $0$, und $u\mapsto u$
bleibt unverändert.
Andererseits wird der Vektor $v$ auf
$v-2v(v\cdot v)=v-2v=-v$ abgebildet.
Vektoren senkrecht auf $v$
werden also belassen, solche parallel zu $v$ werden umgekehrt.

Die Matrix dieser linearen Abbildung ist
\[
S_v=\begin{pmatrix}1&0\\0&1\end{pmatrix}
-
2\begin{pmatrix}v_1\\v_2\end{pmatrix}
\begin{pmatrix}v_1&v_2\end{pmatrix}
=
\begin{pmatrix}
1-2v_1^2&-2v_1v_2\\
-2v_1v_2&1-2v_2^2
\end{pmatrix},
\]
wir wollen nachrechnen, dass sie orthogonal ist.
Dabei beachten
wir, dass $1-v_1^2=v_2^2$ ist, weil ja $|v|=1$, also kann man $S_v$
auch schreiben
\[
S_v=\begin{pmatrix}
v_2^2-v_1^2&-2v_1v_2\\
-2v_1v_2&v_1^2-v_2^2
\end{pmatrix}
\]

\begin{align*}
S_v^tS_v
&=
\begin{pmatrix}
v_2^2-v_1^2&-2v_1v_2\\
-2v_1v_2&v_1^2-v_2^2
\end{pmatrix}
\begin{pmatrix}
v_2^2-v_1^2&-2v_1v_2\\
-2v_1v_2&v_1^2-v_2^2
\end{pmatrix}
\\
&=
\begin{pmatrix}
v_2^4-2v_1^2v_2^2+v_1^4+4v_1^2v_2^2&
-2(v_2^2-v_1^2)v_1v_2
+
-2(v_1^2-v_2^2)v_1v_2
\\
-2(v_1^2-v_2^2)v_1v_2
+
-2(v_2^2-v_1^2)v_1v_2
&
v_2^4-2v_1^2v_2^2+v_1^4+4v_1^2v_2^2
\end{pmatrix}
\\
&=
\begin{pmatrix}
v_1^4+2v_1^2v_2^2+v_2^4&0\\
0&v_1^4+2v_1^2v_2^2+v_2^4\\
\end{pmatrix}
\\
&=
\begin{pmatrix}
(v_1^2+v_2^2)^2&0\\
0&(v_1^2+v_2^2)^2
\end{pmatrix}=E
\end{align*}
Die Matrix $S_v$ ist also tatsächlich immer orthogonal.

Analog gilt in beliebig vielen Dimensionen, dass die Matrix
$S_v=E-2v v^t$  für einen Vektor $v$ der Länge $1$ eine orthogonale
Matrix ist, die die Spiegelung an einer Ebene mit Normale $v$
beschreibt.

\subsubsection{Eigenschaften orthogonaler Matrizen}

\begin{satz}
Die Spalten einer orthogonale Matrix $A$ sind orthonormiert, sie sind
orthogonale Vektoren der Länge 1.
Ausserdem ist $A^{-1}=A^t$.
\end{satz}

\begin{proof}[Beweis]
Multipliziert man $A^tA=E$ von rechts mit $A^{-1}$, bekommt man
$A^tAA^{-1}=A^t=A^{-1}$.

Wir interpretieren die Bedingung $A^tA=E$.
Für ein beliebiges $i$
bedeutet sie, dass Zeile $i$ von $A^t$ mal Spalte $i$ von $A$ $1$ ergibt.
Dies ist aber das Skalarprodukt von Spalte $i$ von $A$ mit Spalte $i$
von $A$, also die Länge vom Spaltenvektor mit der Nummer $i$.

Seien jetzt $i\ne j$ zwei verschiedene Indizes.
Dann bedeutet $A^tA=E$,
dass Zeile $i$ von $A^t$ mal Spalte $j$ von $A$ $0$ ergibt.
Oder das
Skalarprodukt von Spalte $i$ von $A$ mit Spalte $j$ von $A$ ist $0$.
Dies wiederum heisst, dass die Spaltenvektoren senkrecht stehen.
\end{proof}

\subsubsection{Vertauschung der Koordinatenachsen}
Für Transformation, die die Koordinatenachsen vertauscht, ist
die Transformationsmatrix ebenfalls leicht zu ermitteln.
In der Basis
\[
\left\{
b_1'=\begin{pmatrix}0\\0\\1\end{pmatrix},
b_2'=\begin{pmatrix}0\\1\\0\end{pmatrix},
b_3'=\begin{pmatrix}1\\0\\0\end{pmatrix},
\right\}
\]
sind die $x$- und die $z$-Achse vertauscht worden.
die zugehörige Transformationsmatrix ist
\begin{equation}
T=\begin{pmatrix}
0&0&1\\
0&1&0\\
1&0&0
\end{pmatrix}^{-1}
=
\begin{pmatrix}
0&0&1\\
0&1&0\\
1&0&0
\end{pmatrix}.
\label{transformation-vertauschung}
\end{equation}

\subsection{Drehungen des dreidimensionalen Raumes}
\subsubsection{Drehungen um die Koordinatenachsen}
Drehungen um die Koordinatenachsen können mit Hilfe der zweidimensionalen
Drehmatrix $D_\alpha$ gefunden werden.
Die Drehung um die $x$-Achse um den
Winkel $\alpha$ ändert die $x$-Koordinate nicht, hat also die
Matrix
\[
D_{x,\alpha}=
\begin{pmatrix}
1&0&0\\
0&\cos\alpha&-\sin\alpha\\
0&\sin\alpha&\cos\alpha
\end{pmatrix}
\]
Analog können Drehungen um die anderen zwei Achsen formuliert werden:
\[
D_{y,\alpha}=\begin{pmatrix}
\cos\alpha&0&-\sin\alpha\\
0&1&0\\
\sin\alpha&0&\cos\alpha
\end{pmatrix}
,\qquad
D_{z,\alpha}=\begin{pmatrix}
\cos\alpha&-\sin\alpha&0\\
\sin\alpha&\cos\alpha&0\\
0&0&1
\end{pmatrix}
\]
Man kann diese Matrizen aus $D_{x,\alpha}$ aber auch mit Hilfe einer
Koordinatentransformation bekommen.
Die Transformationsmatrix
haben wir in (\ref{transformation-vertauschung}) gefunden:
\[
T=
\begin{pmatrix}
0&0&1\\
0&1&0\\
1&0&0
\end{pmatrix}
=T^{-1}
\]
Mit der Basiswechsel-Formel (\ref{abbildung-basiswechsel}) kann
man jetzt die Drehmatrix um die $z$-Achse aus der Drehmatrix um die
$x$-Achse berechnen:
\begin{align*}
D_{z,-\alpha}=TD_{x,\alpha}T^{-1}
&=
\begin{pmatrix}
0&0&1\\
0&1&0\\
1&0&0
\end{pmatrix}
\begin{pmatrix}
1&0&0\\
0&\cos\alpha&-\sin\alpha\\
0&\sin\alpha&\cos\alpha
\end{pmatrix}
\begin{pmatrix}
0&0&1\\
0&1&0\\
1&0&0
\end{pmatrix}
\\
&=
\begin{pmatrix}
0&\sin\alpha&\cos\alpha\\
0&\cos\alpha&-\sin\alpha\\
1&0&0
\end{pmatrix}
\begin{pmatrix}
0&0&1\\
0&1&0\\
1&0&0
\end{pmatrix}
=
\begin{pmatrix}
\cos\alpha&\sin\alpha&0\\
-\sin\alpha&\cos\alpha&0\\
0&0&1
\end{pmatrix}.
\end{align*}
\subsubsection{Drehwinkel ermitteln}
Für die Standard-Drehmatrizen $D_{x,\alpha}$ ist der Drehwinkel 
leicht mit Hilfe der Spur zu ermitteln.
Die Spur ist die Summe der
Diagonalelemente einer Matrix:
\[
\operatorname{Spur}
\begin{pmatrix}
a_{11}&\dots &a_{1n}\\
\vdots&\ddots&\vdots\\
a_{n1}&\dots &a_{nn}\\
\end{pmatrix}
=a_{11}+a_{22}+\dots+a_{nn}
\quad
\Rightarrow
\quad
\operatorname{Spur}D_{x,\alpha}=1+2\cos\alpha.
\]
Die Spur hat aber interessante algebraische Eigenschaften, welche
die Berechnung des Drehwinkels auch für beliebige Drehmatrizen
erlauben:
\begin{satz}
\label{spursatz}
Seinen $A$, $B$ und $C$ beliebige $n\times n$-Matrizen.
Dann gilt
\begin{compactenum}
\item $\operatorname{Spur}(ABC)=\operatorname{Spur}(BCA)=\operatorname{Spur}(CAB)$
\item $\operatorname{Spur}(AB)=\operatorname{Spur}(BA)$
\end{compactenum}
\end{satz}
Wir zeigen zunächst, wie man damit den Drehwinkel einer beliebigen
Drehung $D$ des dreidimensionalen Raumes bestimmen kann.
Sei $T$ eine Koordinatentransformation, welche die Drehachse der Drehung in die
$x$-Achse transformiert.
Dann hat die transformierte Drehmatrix die Form $D_{x,\alpha}$:
\[
TDT^{-1}= D_{x,\alpha}
\]
Für die Spur gilt dann
\begin{align*}
\operatorname{Spur}(TDT^{-1})
&=
\operatorname{Spur}(DT^{-1}T)
=
\operatorname{Spur}(D)
\\
\operatorname{Spur}(TDT^{-1})
&=
\operatorname{Spur}(D_{x,\alpha})=1+2\cos\alpha
\end{align*}
Aufgelöst nach $\cos\alpha$:
\begin{satz}\label{drehwinkelsatz}
Der Drehwinkel einer Drehung des $\mathbb R^3$ mit Matrix $D$ ist
\[
\cos\alpha =\frac{\operatorname{Spur}(D) -1 }2.
\]
\end{satz}

\begin{proof}[Beweis des Satzes \ref{spursatz}]
Die zweite Aussage folgt aus der ersten, indem man $C=E$ setzt:
\[
\operatorname{Spur}(AB)=\operatorname{Spur}(ABE)=\operatorname{Spur}(BEA)=
\operatorname{Spur}(BA).
\]
Die zyklische Vertauschung der Faktoren folgt aus den Formeln für die
Matrix-Multiplikation.
Das Element $ik$ der Matrix $AB$ ist
\[
\sum_{j=1}^na_{ij}b_{jk}
\]
und entsprechend ist das Matrixelement $il$ von $ABC$ 
\[
\sum_{j=1, k=1}^na_{ij}b_{jk}c_{kl}
\]
Die Diagonalelemente sind jene mit $i=l$, die Spur ist deren Summe:
\[
\operatorname{Spur}(ABC)
=\sum_{i,j,k=1}^na_{ij}b_{jk}c_{ki}
=\sum_{i,j,k=1}^nc_{ki}a_{ij}b_{jk}
=\sum_{j,k,i=1}^nc_{ij}a_{jk}b_{ki}
=\operatorname{Spur}(CAB)
\]
\end{proof}

\subsubsection{Euler-Winkel}
Um die Lage eines Körpers im Raum festzulegen wurde schon früh
die folgende nach Euler benannte  Parametrisierung verwendet.
Die Lage wird durch drei aufeinanderfolgende Drehungen um die $z$-,
die $x$- und dann nochmals die $z$-Achse herbeigeführt.
Die drei
Drehungen sind
\begin{compactenum}
\item eine Drehung um die $z$-Achse um den Winkel $\alpha$
\item eine Drehung um die $x$-Achse um den Winkel $\beta$
\item eine Drehung um die $z$-Achse um den Winkel $\gamma$
\end{compactenum}
Mit der Matrizen-Darstellung der einzelnen Drehungen kann auch
die Matrix der gesamten Drehung berechnet werden.
Wir schreiben
$D_{x,\alpha}$ für eine Drehung um die $x$-Achse um den Winkel
$\alpha$, und sinngemäss für die anderen Achsen.
Es ist
\begin{align*}
O(\alpha,\beta,\gamma)
&=
D_{z,\gamma}
D_{x,\beta}
D_{z,\alpha}
\\
&=
\begin{pmatrix}
\cos\gamma&-\sin\gamma&0\\
\sin\gamma&\cos\gamma&0\\
0&0&1
\end{pmatrix}
\begin{pmatrix}
1&0&0\\
0&\cos\beta&-\sin\beta\\
0&\sin\beta&\cos\beta
\end{pmatrix}
\begin{pmatrix}
\cos\alpha&-\sin\alpha&0\\
\sin\alpha&\cos\alpha&0\\
0&0&1
\end{pmatrix}
\\
&=
\begin{pmatrix}
\cos\gamma&-\sin\gamma&0\\
\sin\gamma&\cos\gamma&0\\
0&0&1
\end{pmatrix}
\begin{pmatrix}
\cos\alpha&-\sin\alpha&0\\
\sin\alpha\cos\beta&\cos\alpha\cos\beta&-\sin\beta\\
\sin\alpha\sin\beta&\cos\alpha\sin\beta&\cos\beta
\end{pmatrix}
\\
&=
\begin{pmatrix}
\cos\alpha\cos\gamma+\sin\alpha\cos\beta\sin\gamma
        &-\sin\alpha\cos\gamma-\cos\alpha\cos\beta\sin\gamma
                &\sin\beta\sin\gamma\\
\cos\alpha\sin\gamma+\sin\alpha\cos\beta\cos\gamma
        &-\sin\alpha\sin\gamma+\cos\alpha\cos\beta\cos\gamma
                &-\sin\beta\cos\gamma\\
\sin\alpha\sin\beta&\cos\alpha\sin\beta&\cos\beta
\end{pmatrix}
\end{align*}
In der Astronomie und der Raumfahrt sind die Winkel sehr gebräuchlich.
Der Winkel $\beta$ ist die Neigung der Bahn gegenüber der Referenzebene,
bei Satellitenbahnen um die Erde ist dies die Äquatorebene,
bei interplanetaren Missionen die Ebene der Erdbahn.
Der Winkel $\gamma$ ist der Winkel zwischen einer Referenzrichtung
und der Schnittgeraden der Bahnebene mit der Referenzebene.
Diese Schnittgerade heisst auch die Knotenlinie.
Bei Planetenbahnen ist die Referenzrichtung die Richtung des Frühlingspunktes.
Der Winkel $\alpha$ ist der Winkel zwischen der Knotenlinie und dem
erd- oder sonnennächsten Punkt.



\subsection{Orthonormalisierung}
\index{Orthonormalisierung}
\index{Gram-Schmidt-Prozess|see{Orthonormalisierung}}
Orthogonale Vektoren sind besonders gut als Basis geeignet, weil
sich dann das Skalarprodukt am einfachsten ausdrücken lässt.
Dies ist der eigentliche Inhalt des Satzes von Pythagoras: wenn
\index{Pythagoras}
man die Achsen orthogonal wählt, kann man die Länge mit Hilfe
einer Quadratsumme der Koordinaten ausrechnen.
Daher stellt sich 
das Problem, aus einer gegebenen Menge von Vektoren eine Menge
von orthogonalen Vektoren zu machen.

Sei also $V=\{b_1,\dots,b_n\}$ eine linear unabhängige Menge von
Vektoren.
Gesucht ist eine ebenfalls linear unabhängige Menge 
von Vektoren $B'=\{b_1',\dots,b_n'\}$, die den gleichen Unterraum
aufspannen.
Ausserdem soll gelten:
\begin{enumerate}
\item $b_i'^tb_j'=0$ falls $i\ne j$: Vektoren von $B'$ sind orthogonal.
\item $|b_i'|=1$ für alle $i$: Vektoren von $B'$ sind normiert.
\end{enumerate}
Eine Menge von Vektoren $B'$ mit diesen Eigenschaften heisst
{\it orthonormiert}.
Man beachte, dass $B$ keine Basis sein muss, die Vektoren sind zwar
linear unabhängig, müssen aber nicht den ganzen Vektorraum erzeugen.
Falls $B$ eine Basis ist, dann wird auch $B'$ eine Basis sein, beide
Vektormengen erzeugen den ganzen Raum.

Wir konstruieren jetzt schrittweise die Menge $B'$.
Der Vektor $b_1$ hat als schlimmsten möglichen Fehler, dass er
nicht die richtige Länge hat, also setzen wir
\[
b_1' = \frac{b_1}{|b_1|}.
\]
Der zweite Vektor $b_2$ hat möglicherweise nicht die richtige Länge,
aber noch viel schwerer wiegt, dass er nicht senkrecht auf $b_1'$
steht.
Man kann jedoch die zu $b_1'$ parallele Komponente mit Hilfe
des Skalarproduktes finden: $(b_1'\cdot b_2)b_1'$ ist ein Vektor
parallel zu $b_1'$.
Subtrahieren wir dieses Stück von $b_2$ bekommen
wir einen Vektor, der senkrecht auf $b_1'$ steht:
\[
b_1'\cdot(b_2-(b_1'\cdot b_2)b_1')=
b_1'\cdot b_2-(b_1'\cdot b_2)(\underbrace{b_1'\cdot b_1'}_{=1})
=
b_1'\cdot b_2-
b_1'\cdot b_2=0.
\]
Bringen wir ihn noch auf Länge $1$, bekommen wir den Vektor
\[
b_2'=\frac{
b_2-(b_2\cdot b_1')b_1'
}{
|b_2-(b_2\cdot b_1')b_1'|
}
\]
Im nächsten Schritt gehen wir im Prinzip gleich vor, müssen aber
auch noch $b_2$ hinzuziehen:
\begin{align*}
b_3'&=\frac{
b_3-(b_3\cdot b_1')b_1'-(b_3\cdot b_2')b_2'
}{
|b_3-(b_3\cdot b_1')b_1'-(b_3\cdot b_2')b_2'|
}
\\
&\vdots
\\
b_n'&=\frac{
b_n-(b_n\cdot b_1')b_1'-\dots-(b_n\cdot b_{n-1}')b_{n-1}'
}{
|b_n-(b_n\cdot b_1')b_1'-\dots-(b_n\cdot b_{n-1}')b_{n-1}'|
}
\end{align*}

Damit haben wir einen rekursiven Algorithmus gefunden,
der das gestellt Problem löst:

\begin{satz}[Gram-Schmidt] Ist $B$ eine linear unabhängige Menge von Vektoren,
dann gibt es eine orthonormierte Menge von Vektoren $B'$ so, dass
$\langle b_1,\dots,b_k\rangle=\langle b_1',\dots,b_k'\rangle$.
Die
Vektoren $b_i'$ sind
\begin{align*}
b_1'&=\frac{b_1}{|b_1|}\\
b_k'&=\frac{b_k-(b_k\cdot b_1')b_1'-\dots -(b_k\cdot b_{k-1}')b_{k-1}'}%
{|b_k-(b_k\cdot b_1')b_1'-\dots -(b_k\cdot b_{k-1}')b_{k-1}'|}
\end{align*}
\end{satz}

\begin{beispiel}
Man orthonormalisiere die Menge 
\[
B=\left\{
b_1=\begin{pmatrix}1\\1\\1\end{pmatrix},
b_2=\begin{pmatrix}0\\1\\1\end{pmatrix},
b_3=\begin{pmatrix}0\\0\\1\end{pmatrix}
\right\}.
\]
Die Rekursionsformeln liefern nacheinander
\begin{align*}
b_1'&=\frac{b_1}{|b_1|}=\frac1{\sqrt{3}}\begin{pmatrix}1\\1\\1\end{pmatrix}
\\
b_2\cdot b_1'&=\frac2{\sqrt{3}}
\\
b_2'&=\frac{
b_2-\frac2{\sqrt{3}}b_1'
}{
|b_2-\frac2{\sqrt{3}}b_1'|
}
=
\frac{
\begin{pmatrix}0\\1\\1\end{pmatrix}-\frac23\begin{pmatrix}1\\1\\1\end{pmatrix}
}{
|b_2-\frac2{\sqrt{3}}b_1'|
}
=
\frac{\begin{pmatrix}-\frac23\\\frac13\\\frac13\end{pmatrix}}{
\sqrt{\frac19+\frac19+\frac49}}
=\frac1{\sqrt{6}}\begin{pmatrix}-2\\1\\1\end{pmatrix}
\\
b_3\cdot b_1'&=\frac1{\sqrt{3}}
\\
b_3\cdot b_2'&=\frac1{\sqrt{6}}
\\
b_3'&=\frac{
b_3-(b_3\cdot b_1')b_1'-(b_3\cdot b_2')b_2'
}{
|b_3-(b_3\cdot b_1')b_1'-(b_3\cdot b_2')b_2'|
}
=
\frac{
\begin{pmatrix}0\\0\\1\end{pmatrix}
-\frac1{\sqrt{3}}\cdot\frac1{\sqrt{3}}\begin{pmatrix}1\\1\\1\end{pmatrix}
-\frac1{\sqrt{6}}\cdot\frac1{\sqrt{6}}\begin{pmatrix}-2\\1\\1\end{pmatrix}
}{
|b_3-(b_3\cdot b_1')b_1'-(b_3\cdot b_2')b_2'|
}
\\
&
=
\frac{
\begin{pmatrix}0\\0\\1\end{pmatrix}
-\frac13\begin{pmatrix}1\\1\\1\end{pmatrix}
-\frac16\begin{pmatrix}-2\\1\\1\end{pmatrix}
}{
|b_3-(b_3\cdot b_1')b_1'-(b_3\cdot b_2')b_2'|
}
=
\frac{
\begin{pmatrix}0\\-\frac12\\\frac12\end{pmatrix}
}{
\sqrt{\frac14+\frac14}
}
=\sqrt{2}
\begin{pmatrix}0\\-\frac12\\\frac12\end{pmatrix}
\end{align*}
Die gesuchte Menge $B'$ ist also
\[
B'=\left\{
b_1'=\frac1{\sqrt{3}}\begin{pmatrix}1\\1\\1\end{pmatrix},
b_2'=\frac1{\sqrt{6}}\begin{pmatrix}-2\\1\\1\end{pmatrix},
b_3'=\frac1{\sqrt{2}}\begin{pmatrix}0\\-1\\1\end{pmatrix}
\right\}
\]
Man kann leicht verifizieren, dass diese Menge die verlangten
Orthogonalitätseigenschaften erfüllt, die Vektoren haben
alle die Länge $1$ und sie sind paarweise senkrecht.
\end{beispiel}

\section{Überbestimmte Gleichungssysteme\label{section:ueberbestimmt}}
\index{Gleichungssystem!ueberbestimmtes@\überbestimmtes}
Überbestimmte Gleichungssysteme sind Gleichungssysteme der Form
$Ax=b$ mit einer $m\times n$-Matrix mit $m>n$, also mehr Gleichungen
als Unbekannten.
Im allgemeinen sind sie nicht lösbar, weil $b$ nicht
im Bild von $A$ enthalten ist: $b\not\in \operatorname{im}A$.

Statt einer exakten Lösung könnte man daher eine approximative
Lösung suchen, welche die Gleichung möglichst gut erfüllt,
der Vektor $Ax-b$ sollte also möglichst kurz sein.
Geometrisch
geht es also darum, das Lot vom Punkt $b$ auf den von den Spaltenvektoren
von $A$ aufgespannten Unterraum zu fällen.
Wir suchen also
einen Vektor, der auf allen Spaltenvektoren von $A$ senkrecht steht.
Das Skalarprodukt von Spaltenvektoren von $A$ mit $Ax-b$ ist aber
$A^t(Ax-b)$, wir müssen also das Gleichungssystem
\[A^t(Ax-b)=0\]
lösen.
Nach Ausmultiplizieren bekommen wir
\begin{equation}
A^tAx-A^tb=0\quad\Rightarrow\quad A^tAx=A^tb\quad\Rightarrow\quad
x=(A^tA)^{-1}A^tb.
\label{uberbestimmt}
\end{equation}
Man beachte, dass $A$ nicht quadratisch ist, und dass man daher
nicht mit $(A^tA)^{-1}A^t=A^{-1}(A^t)^{-1}A^t=A^{-1}$ vereinfachen
kann.

\begin{beispiel}
Sei 
\[
A=\begin{pmatrix}1\\1\\1\\\end{pmatrix},\quad b=\begin{pmatrix}1\\2\\3\end{pmatrix}.
\]
Offenbar ist $b\not\in\operatorname{im}A$.
Nach der Formel (\ref{uberbestimmt}) muss man zunächst $A^tA$ ausrechnen:
\[
A^tA=\begin{pmatrix}1&1&1\end{pmatrix}\begin{pmatrix}1\\1\\1\end{pmatrix}=3.
\]
Damit kann man jetzt nach (\ref{uberbestimmt}) die bestmögliche
approximative Lösung finden:
\[
x=\frac13\cdot\begin{pmatrix}1&1&1\end{pmatrix}
\begin{pmatrix}1\\2\\3\end{pmatrix}=2.
\]
Der von $b$ am wenigsten weit entfernte Punkt der Geraden mit
Richtung $A$ ist also der Punkt $(2,2,2)$.
\end{beispiel}
